<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>AI Stack Dashboard</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #4a6cf7;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-bg: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --success-color: #28a745;
            --warning-color: #ffc107;
            --danger-color: #dc3545;
            --header-bg: linear-gradient(135deg, var(--primary-color) 0%, #3a5bd9 100%);
            --header-text: #ffffff;
            --integration-title-color: #212529;
            --integration-title-border: none;
            --link-color: #0d6efd;
            --link-hover-color: #0a58ca;
            --card-shadow: rgba(0, 0, 0, 0.1);
            --btn-text: #212529;
            --code-bg: #f5f5f5;
            --important-note-bg: #fff3cd;
            --important-note-border: #ffc107;
            --important-note-text: #856404;
        }

        [data-theme="dark"] {
            --primary-color: #6c7ed6;
            --secondary-color: #adb5bd;
            --background-color: #212529;
            --card-bg: #343a40;
            --text-color: #f8f9fa;
            --border-color: #495057;
            --success-color: #28a745;
            --warning-color: #ffc107;
            --danger-color: #dc3545;
            --header-bg: linear-gradient(135deg, #343a40 0%, #212529 100%);
            --header-text: #f8f9fa;
            --integration-title-color: #f8f9fa;
            --integration-title-border: none;
            --link-color: #8bb9fe;
            --link-hover-color: #afd2fe;
            --card-shadow: rgba(0, 0, 0, 0.5);
            --btn-text: #f8f9fa;
            --code-bg: #2b3035;
            --important-note-bg: #3a3a1c;
            --important-note-border: #6b6b2b;
            --important-note-text: #e9e9a8;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--background-color);
            color: var(--text-color);
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        a:hover {
            color: var(--link-hover-color);
            text-decoration: underline;
        }

        /* Override hover styles for buttons and service links */
        .btn:hover, .service-link:hover, .docs-btn:hover {
            color: white;
            text-decoration: none;
        }

        .container {
            width: 95%;
            max-width: 1800px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: var(--header-bg);
            color: var(--header-text);
            padding: 15px 0;
            margin-bottom: 20px;
            border-radius: 0 0 10px 10px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
            position: sticky;
            top: 0;
            z-index: 1000;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, rgba(255,255,255,0) 70%);
            opacity: 0.7;
            z-index: 1;
        }

        header .container {
            position: relative;
            z-index: 2;
        }

        .header-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .header-controls {
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .theme-toggle-btn {
            background: none;
            border: none;
            cursor: pointer;
            font-size: 1.2rem;
            color: var(--header-text);
            padding: 5px;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.3s ease;
            position: relative;
        }

        .theme-toggle-btn:hover {
            transform: rotate(15deg);
        }

        .dark-icon {
            display: inline-block;
        }

        .light-icon {
            display: none;
        }

        [data-theme="dark"] .dark-icon {
            display: none;
        }

        [data-theme="dark"] .light-icon {
            display: inline-block;
        }

        h1 {
            margin: 0;
            font-size: 2rem;
            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
            letter-spacing: 1px;
        }

        .subtitle {
            font-size: 1rem;
            opacity: 0.9;
            margin: 0 20px;
            font-weight: 300;
            white-space: nowrap;
        }

        .ip-info-box, .integration-box {
            background-color: var(--card-bg);
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-bottom: 30px;
        }

        .ip-info-box {
            border-top: 4px solid #2980b9;
        }

        .integration-box {
            border-top: 4px solid #8e44ad;
        }

        .integration-box h3 {
            margin-top: 0;
            color: var(--integration-title-color);
            border-bottom: var(--integration-title-border);
            padding-bottom: 10px;
            margin-bottom: 15px;
        }

        .integration-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
        }

        .header-title {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .collapse-btn {
            background: none;
            border: none;
            cursor: pointer;
            font-size: 1.2em;
            color: var(--text-color);
            padding: 5px;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.3s ease;
        }

        .collapse-btn:hover {
            color: var(--primary-color);
        }

        .collapse-btn.collapsed .collapse-icon {
            transform: rotate(-90deg);
        }

        .collapsible-content {
            transition: max-height 0.5s ease;
            overflow: hidden;
            max-height: 10000px; /* Large enough to contain all content */
        }

        .collapsible-content.collapsed {
            max-height: 0;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }

        .service-selector {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .service-btn {
            padding: 6px 12px;
            background-color: rgba(0, 0, 0, 0.05);
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.3s ease;
            color: var(--text-color);
        }

        .service-btn.active {
            background-color: #2980b9;
            color: white;
        }

        .service-btn:hover:not(.active) {
            background-color: rgba(0, 0, 0, 0.1);
        }

        .integration-selection-info {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            padding: 0 10px;
            text-align: center;
        }

        .integration-intro {
            margin-top: 10px;
            margin-bottom: 15px;
            font-style: italic;
            color: #666;
            text-align: center;
            width: 100%;
        }

        .service-selector-wrapper {
            display: flex;
            justify-content: center;
            margin: 15px 0;
            width: 100%;
        }

        .service-dropdown {
            width: 100%;
            max-width: 400px;
            padding: 10px 15px;
            border: 2px solid var(--primary-color);
            border-radius: 8px;
            background-color: var(--card-bg);
            color: var(--text-color);
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            appearance: none;
            -webkit-appearance: none;
            -moz-appearance: none;
            background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="12" height="6"><path d="M0 0h12L6 6z" fill="%234a6cf7"/></svg>');
            background-repeat: no-repeat;
            background-position: right 15px center;
            background-size: 12px;
        }

        .service-dropdown:focus {
            outline: none;
            box-shadow: 0 0 0 3px rgba(74, 108, 247, 0.3);
        }

        .select-service-title {
            text-align: center;
            margin-bottom: 5px;
            color: var(--text-color);
            font-weight: 600;
        }

        .service-selector-inline {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            margin-bottom: 5px;
        }

        .integration-tabs {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-bottom: 15px;
        }

        .integration-tab {
            flex: 1;
            padding: 10px 15px;
            background-color: rgba(0, 0, 0, 0.05);
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.3s ease;
            text-align: center;
            min-width: 150px;
            color: var(--text-color);
        }

        .integration-tab.active {
            background-color: #8e44ad;
            color: white;
        }

        .integration-tab:hover:not(.active) {
            background-color: rgba(0, 0, 0, 0.1);
        }

        .integration-content {
            display: none;
            padding: 15px;
            background-color: var(--card-bg);
            border-radius: 5px;
        }

        .important-note {
            background-color: rgba(74, 108, 247, 0.1);
            border-left: 4px solid var(--primary-color);
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }

        .important-note h5 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.1rem;
        }

        .important-note ul {
            margin-bottom: 0;
        }

        .model-table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.9rem;
        }

        .model-table th {
            background-color: var(--primary-color);
            color: white;
            text-align: left;
            padding: 10px;
        }

        .model-table td {
            border: 1px solid var(--border-color);
            padding: 8px 10px;
        }

        .model-table tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.03);
        }

        .model-table tr:hover {
            background-color: rgba(74, 108, 247, 0.05);
        }

        .integration-content.active {
            display: block;
        }

        .integration-content h4 {
            margin-top: 0;
            color: #8e44ad;
        }

        .integration-content ol {
            padding-left: 20px;
        }

        .integration-content li {
            margin-bottom: 8px;
        }

        .integration-content pre {
            background-color: var(--code-bg);
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 10px 0;
        }

        .integration-content pre code {
            font-family: monospace;
            white-space: pre-wrap;
            font-size: 0.9em;
        }

        .important-note {
            background-color: var(--important-note-bg);
            border-left: 4px solid var(--important-note-border);
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .important-note h5 {
            margin-top: 0;
            color: var(--important-note-text);
            font-size: 1.1em;
        }

        .important-note ul {
            margin-bottom: 0;
            padding-left: 20px;
        }

        .important-note li {
            margin-bottom: 5px;
        }

        .ip-info-content {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            justify-content: space-between;
        }

        .ip-section {
            flex: 1;
            min-width: 250px;
        }

        .ip-section h3 {
            color: var(--primary-color);
            margin-top: 0;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 10px;
            margin-bottom: 15px;
        }

        .note {
            font-size: 0.85rem;
            color: #666;
            margin-top: 10px;
            font-style: italic;
        }

        .ip-buttons {
            display: flex;
            gap: 10px;
            margin-top: 10px;
        }

        .refresh-ip-btn {
            background-color: #2980b9;
            color: white;
        }

        .refresh-ip-btn:hover {
            background-color: #3498db;
            transform: translateY(-2px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        }

        .hide-ip-btn {
            background-color: #e74c3c;
            color: white;
        }

        .hide-ip-btn:hover {
            background-color: #c0392b;
            transform: translateY(-2px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        }

        .ip-hidden {
            filter: blur(5px);
        }

        .ip-hidden:hover {
            filter: none;
        }

        .services-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
            width: 100%;
        }

        .service-card {
            background-color: var(--card-bg);
            border-radius: 10px;
            box-shadow: 0 4px 6px var(--card-shadow);
            padding: 20px;
            transition: transform 0.3s ease, box-shadow 0.3s ease, border-top-color 0.3s ease;
            position: relative;
            overflow: hidden;
            border-top: 4px solid var(--primary-color);
            display: flex;
            flex-direction: column;
            min-height: 350px; /* Set a minimum height for consistency */
        }

        .service-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px var(--card-shadow);
            border-top-color: var(--secondary-color);
        }

        .service-card::before {
            content: '';
            position: absolute;
            top: -4px;
            left: 0;
            width: 100%;
            height: 4px;
            background: linear-gradient(90deg, var(--primary-color), var(--secondary-color));
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .service-card:hover::before {
            opacity: 1;
        }

        .service-card h3 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.5rem;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 10px;
        }

        .service-card p {
            margin-bottom: 15px;
            color: var(--secondary-color);
        }

        .card-content {
            flex: 1;
            display: flex;
            flex-direction: column;
        }

        .service-info {
            margin-top: auto;
            text-align: left;
            padding-top: 10px;
            font-size: 0.9rem;
        }

        .service-info p:first-of-type {
            border-top: 1px dashed var(--border-color);
            padding-top: 10px;
            margin-top: 5px;
        }

        .service-info p:last-of-type {
            border-bottom: 1px dashed var(--border-color);
            padding-bottom: 10px;
            margin-bottom: 5px;
        }

        .port-protocol {
            font-size: 0.8rem;
            color: #333;
            font-weight: normal;
            margin-left: 5px;
        }

        .copyable {
            cursor: pointer;
            position: relative;
            padding: 2px 5px;
            background-color: rgba(0, 0, 0, 0.05);
            border-radius: 3px;
            transition: background-color 0.3s ease;
        }

        .copyable:hover {
            background-color: rgba(0, 0, 0, 0.1);
        }

        .copyable:hover::after {
            content: '📋';
            position: absolute;
            right: -20px;
            top: 50%;
            transform: translateY(-50%);
            font-size: 14px;
        }

        .copyable.copied {
            background-color: rgba(40, 167, 69, 0.2);
        }

        .copyable.copied::after {
            content: '✓';
            position: absolute;
            right: -20px;
            top: 50%;
            transform: translateY(-50%);
            color: #28a745;
            font-weight: bold;
        }

        .card-buttons {
            margin-top: auto;
            padding-top: 15px;
            display: flex;
            flex-direction: column;
            width: 100%;
        }

        .service-links {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            gap: 10px;
        }

        .service-link {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 8px 16px;
            background-color: var(--primary-color);
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease, box-shadow 0.3s ease;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            height: 40px;
            box-sizing: border-box;
            white-space: nowrap;
            min-width: 120px;
        }

        .service-link:hover {
            background-color: #3a5bd9;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            color: white;
            text-decoration: none;
        }

        .http-link {
            background-color: #2980b9; /* Blue color */
            position: relative;
            overflow: hidden;
            z-index: 1;
        }

        .http-link::before {
            content: '🌐';
            margin-right: 6px;
            font-size: 1.1em;
        }

        .http-link:hover {
            background-color: #3498db;
            color: white;
            text-decoration: none;
        }

        .https-link {
            background-color: #28a745; /* Green color */
            position: relative;
            overflow: hidden;
            z-index: 1;
        }

        .https-link::before {
            content: '🔒';
            margin-right: 6px;
            font-size: 1.1em;
        }

        .https-link:hover {
            background-color: #218838;
            color: white;
            text-decoration: none;
        }

        .docs-link {
            background-color: var(--warning-color);
            color: var(--text-color);
            display: inline-flex;
            align-items: center;
            margin-top: 10px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        .docs-link:hover {
            background-color: #e0a800;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        /* Floating navigation buttons */
        .floating-nav {
            position: fixed;
            right: 20px;
            bottom: 20px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            z-index: 1000;
        }

        .floating-nav button {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background-color: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.2rem;
            transition: background-color 0.3s, transform 0.2s;
        }

        .floating-nav button:hover {
            background-color: var(--secondary-color);
            transform: scale(1.1);
        }

        footer {
            text-align: center;
            margin-top: 30px;
            padding: 20px 0;
            border-top: 1px solid var(--border-color);
            color: var(--secondary-color);
        }

        .status-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 5px;
        }

        .running {
            background-color: var(--success-color);
        }

        .stopped {
            background-color: var(--danger-color);
        }

        .service-status {
            position: absolute;
            top: 20px;
            right: 20px;
            font-size: 0.8rem;
            display: flex;
            align-items: center;
        }

        .service-controls {
            display: flex;
            justify-content: center;
            gap: 8px;
            margin-top: 10px;
        }

        .btn {
            padding: 8px 12px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.85rem;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease, box-shadow 0.3s ease;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        }

        .start-btn {
            background-color: var(--success-color);
            color: white;
        }

        .start-btn:hover {
            background-color: #218838;
        }

        .restart-btn {
            background-color: var(--warning-color);
            color: var(--text-color);
        }

        .restart-btn:hover {
            background-color: #e0a800;
        }

        .stop-btn {
            background-color: var(--danger-color);
            color: white;
        }

        .stop-btn:hover {
            background-color: #bd2130;
        }

        .docs-btn {
            background-color: #8e44ad; /* Purple color to stand out */
            color: white;
            text-decoration: none;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 15px 0;
            padding: 8px 15px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            border: 2px solid #8e44ad;
            border-radius: 4px;
            font-weight: bold;
            position: relative;
            overflow: hidden;
            z-index: 1;
            width: calc(100% - 4px); /* Account for the border */
            box-sizing: border-box;
            text-align: center;
            height: 40px;
        }

        .docs-btn::before {
            content: '📚';
            margin-right: 6px;
            font-size: 1.1em;
        }

        .docs-btn::after {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, rgba(255,255,255,0.2), transparent);
            transition: left 0.5s ease;
            z-index: -1;
        }

        .docs-btn:hover {
            background-color: #9b59b6;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
            border-color: #9b59b6;
            color: white;
            text-decoration: none;
        }

        .docs-btn:hover::after {
            left: 100%;
        }

        /* YouTube Subscribe Button */
        .youtube-subscribe {
            margin: 0;
        }

        .youtube-btn {
            display: inline-flex;
            align-items: center;
            background-color: #FF0000;
            color: white;
            font-weight: bold;
            padding: 8px 12px;
            border-radius: 4px;
            text-decoration: none;
            transition: background-color 0.3s ease, transform 0.2s ease;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            font-size: 0.9rem;
        }

        .youtube-btn:hover {
            background-color: #CC0000;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }

        .youtube-icon {
            font-style: normal;
            margin-right: 6px;
            font-size: 12px;
        }

        .author-link {
            color: var(--header-text);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .author-link:hover {
            color: rgba(255, 255, 255, 0.8);
            text-decoration: underline;
        }

        @media (max-width: 1200px) {
            .services-grid {
                grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
            }

            .service-card {
                min-height: 380px;
            }
        }

        @media (max-width: 992px) {
            .header-content {
                flex-direction: column;
                text-align: center;
                gap: 10px;
            }

            .header-controls {
                margin-top: 10px;
            }

            .integration-header {
                flex-direction: column;
                gap: 15px;
            }

            .integration-selection-info {
                gap: 10px;
            }

            .integration-intro {
                margin-bottom: 10px;
            }
        }

        @media (max-width: 768px) {
            .services-grid {
                grid-template-columns: 1fr;
            }

            .youtube-btn {
                padding: 8px 12px;
                font-size: 14px;
            }

            .service-card {
                min-height: 320px;
            }

            .ip-info-content {
                flex-direction: column;
            }

            .ip-section {
                width: 100%;
            }
        }

        @media (max-width: 576px) {
            .service-links {
                flex-direction: column;
                gap: 10px;
            }

            .service-link {
                width: 100%;
            }

            .service-controls {
                flex-wrap: wrap;
                justify-content: center;
            }

            .integration-tabs {
                flex-direction: column;
            }

            .integration-tab {
                width: 100%;
            }
        }

        @media (min-width: 2000px) {
            .services-grid {
                grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
            }

            body {
                font-size: 18px;
            }

            .service-card {
                min-height: 420px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <h1>iTaK AI Stack</h1>
                <p class="subtitle">Your self-hosted AI infrastructure <a href="https://www.youtube.com/@David-Patton" target="_blank" class="author-link">created by David Patton</a></p>
                <div class="header-controls">
                    <button id="theme-toggle" class="theme-toggle-btn" onclick="toggleTheme()">
                        <span class="light-icon">☀️</span>
                        <span class="dark-icon">🌙</span>
                    </button>
                    <div class="youtube-subscribe">
                        <a href="https://www.youtube.com/@David-Patton?sub_confirmation=1" target="_blank" class="youtube-btn">
                            <i class="youtube-icon">▶</i> SUBSCRIBE
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="ip-info-box">
            <div class="integration-header">
                <div class="header-title">
                    <h3>Network Information</h3>
                    <button class="collapse-btn" onclick="toggleCollapse('ip-info-content-wrapper')"><span class="collapse-icon">▼</span></button>
                </div>
            </div>
            <div id="ip-info-content-wrapper" class="collapsible-content">
                <div class="ip-info-content">
                <div class="ip-section">
                    <h3>Your External IP</h3>
                    <p><code class="copyable" id="external-ip" data-copy="">Loading...</code></p>
                    <div class="ip-buttons">
                        <button class="btn refresh-ip-btn" onclick="refreshExternalIP()">Refresh</button>
                        <button class="btn hide-ip-btn" onclick="toggleHideIP()">Hide IP</button>
                    </div>
                </div>
                <div class="ip-section">
                    <h3>Dynamic DNS</h3>
                    <p><strong>Your Domain:</strong> <code class="copyable" id="dynamic-dns" data-copy="example.ddns.net">example.ddns.net</code></p>
                    <p class="note">Visit <a href="https://www.noip.com/" target="_blank">NoIP.com</a> to set up a free dynamic DNS service that will keep your domain pointing to your changing IP address.</p>
                </div>
                <div class="ip-section">
                    <h3>Configuration</h3>
                    <p>Future updates will allow you to set your username, password, email, and dynamic DNS settings.</p>
                </div>
            </div>
        </div>

        <div class="integration-box">
            <div class="integration-header">
                <div class="header-title">
                    <h3>Service Integrations</h3>
                    <button class="collapse-btn" onclick="toggleCollapse('integration-content-wrapper')"><span class="collapse-icon">▼</span></button>
                </div>
            </div>
            <div id="integration-content-wrapper" class="collapsible-content">
                <div class="integration-selection-info">
                    <h4 class="select-service-title">Select a service to explore integration possibilities:</h4>
                    <div class="service-selector-wrapper">
                        <select id="service-dropdown" class="service-dropdown" onchange="selectServiceFromDropdown()">
                            <option value="" disabled selected>-- Select a service --</option>
                            <option value="n8n">n8n - Workflow Automation</option>
                            <option value="flowise">Flowise - Visual AI Builder</option>
                            <option value="ollama">Ollama - Local LLM Runner</option>
                            <option value="huggingface">Hugging Face - Chat UI</option>
                            <option value="searxng">SearXNG - Private Search Engine</option>
                            <option value="youtube">YouTube Downloader</option>
                            <option value="playwright">Playwright - Web Scraper</option>
                            <option value="portainer">Portainer - Container Management</option>
                            <option value="supabase">Supabase - Database & Backend</option>
                            <option value="mcp">MCP - Model Context Protocol</option>

                            <option value="advanced">Advanced Integrations</option>
                            <option value="aibot">AI Bot Types</option>
                        </select>
                    </div>
                    <p class="integration-intro">Learn how to combine services to create powerful AI workflows on your local machine.</p>
                </div>
                <div class="integration-tabs" id="n8n-tabs" style="display: block;">
                    <button class="integration-tab active" onclick="showIntegration('n8n-flowise')">n8n + Flowise</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-ollama')">n8n + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-searxng')">n8n + SearXNG</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-youtube')">n8n + YouTube</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-playwright')">n8n + Playwright</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-discord')">n8n + Discord</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-huggingface-discord')">n8n + Hugging Face + Discord</button>
                    <button class="integration-tab" onclick="showIntegration('n8n-import')">Import n8n Workflows</button>
                </div>
                <div class="integration-tabs" id="flowise-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('flowise-n8n')">Flowise + n8n</button>
                    <button class="integration-tab" onclick="showIntegration('flowise-ollama')">Flowise + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('flowise-searxng')">Flowise + SearXNG</button>
                    <button class="integration-tab" onclick="showIntegration('flowise-youtube')">Flowise + YouTube</button>
                    <button class="integration-tab" onclick="showIntegration('flowise-discord')">Flowise + Discord</button>
                    <button class="integration-tab" onclick="showIntegration('flowise-import')">Import Flowise Chatflows</button>
                </div>

                <div class="integration-tabs" id="ollama-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('ollama-flowise')">Ollama + Flowise</button>
                    <button class="integration-tab" onclick="showIntegration('ollama-models')">Managing Ollama Models</button>
                </div>

                <div class="integration-tabs" id="huggingface-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('huggingface-ollama')">Hugging Face + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('huggingface-customization')">Customizing the Chat UI</button>
                </div>

                <div class="integration-tabs" id="searxng-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('searxng-ollama')">SearXNG + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('searxng-customization')">Customizing SearXNG</button>
                </div>

                <div class="integration-tabs" id="youtube-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('youtube-ollama')">YouTube + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('youtube-batch')">Batch Processing Videos</button>
                </div>

                <div class="integration-tabs" id="playwright-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('playwright-flowise')">Playwright + Flowise</button>
                </div>

                <div class="integration-tabs" id="portainer-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('portainer-management')">Managing Your Services</button>
                </div>

                <div class="integration-tabs" id="supabase-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('supabase-setup')">Setting Up Your Database</button>
                </div>

                <div class="integration-tabs" id="advanced-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('advanced-multiservice')">Multi-Service AI Workflows</button>
                    <button class="integration-tab" onclick="showIntegration('advanced-rag')">Building RAG Systems</button>
                </div>

                <div class="integration-tabs" id="aibot-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('aibot-text')">Text Reading Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-pdf')">PDF Reading Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-webscraping')">Web Scraping Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-video')">Video Analysis Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-vision')">Computer Vision Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-multimodal')">Multi-Modal Bot</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-finetuning')">Fine-Tuning Models</button>
                    <button class="integration-tab" onclick="showIntegration('aibot-storage')">Bot Storage & Memory</button>
                </div>

                <div class="integration-tabs" id="mcp-tabs" style="display: none;">
                    <button class="integration-tab" onclick="showIntegration('mcp-intro')">MCP Introduction</button>
                    <button class="integration-tab" onclick="showIntegration('mcp-ollama')">MCP + Ollama</button>
                    <button class="integration-tab" onclick="showIntegration('mcp-huggingface')">MCP + Hugging Face</button>
                    <button class="integration-tab" onclick="showIntegration('mcp-apps')">Building MCP Apps</button>
                    <button class="integration-tab" onclick="showIntegration('mcp-itak')">MCP + iTaK Services</button>
                </div>

                
                <!-- MCP Introduction -->
<div class="integration-content" id="mcp-intro">
    <h4>Introduction to Model Context Protocol (MCP)</h4>
    <p>Model Context Protocol (MCP) is a standardized framework that enables AI models to access external tools and capabilities, significantly enhancing their ability to perform real-world tasks.</p>

    <div class="important-note">
        <h5>🌟 What is MCP?</h5>
        <p>Model Context Protocol (MCP) is a powerful framework that:</p>
        <ul>
            <li>Connects AI models to external tools like web search, code execution, and databases</li>
            <li>Enables models to retrieve real-time information beyond their training cutoff dates</li>
            <li>Allows AI to perform actions in the real world through controlled interfaces</li>
            <li>Provides a consistent interface across different AI models and platforms</li>
            <li>Enhances AI capabilities while maintaining security and privacy controls</li>
        </ul>
    </div>

    <p><strong>Core Components of MCP:</strong></p>
    <ul>
        <li><strong>Context Engine:</strong> Manages the flow of information between the model and external tools</li>
        <li><strong>Tool Registry:</strong> Catalogs available tools and their capabilities</li>
        <li><strong>Security Layer:</strong> Controls permissions and access to sensitive operations</li>
        <li><strong>Execution Environment:</strong> Where tool operations are safely performed</li>
    </ul>

    <p><strong>Standard MCP Tools:</strong></p>
    <table class="model-table">
        <tr>
            <th>Tool Category</th>
            <th>Capabilities</th>
            <th>Example Applications</th>
        </tr>
        <tr>
            <td>Information Retrieval</td>
            <td>Web search, knowledge base access, document retrieval</td>
            <td>Research assistance, fact-checking, current events</td>
        </tr>
        <tr>
            <td>Computation</td>
            <td>Code execution, data analysis, mathematical operations</td>
            <td>Data science, problem-solving, simulations</td>
        </tr>
        <tr>
            <td>Content Creation</td>
            <td>Image generation, document creation, code synthesis</td>
            <td>Design assistance, content production, programming</td>
        </tr>
        <tr>
            <td>System Integration</td>
            <td>API connections, database access, file operations</td>
            <td>Workflow automation, data management, system control</td>
        </tr>
    </table>

    <p><strong>Benefits of MCP:</strong></p>
    <ul>
        <li><strong>Enhanced Accuracy:</strong> Models can verify information rather than relying on training data</li>
        <li><strong>Extended Capabilities:</strong> AI can perform complex tasks beyond text generation</li>
        <li><strong>Real-time Information:</strong> Access to current data eliminates knowledge cutoff limitations</li>
        <li><strong>Practical Utility:</strong> Enables AI to solve real-world problems more effectively</li>
        <li><strong>Controlled Environment:</strong> Security measures prevent unauthorized actions</li>
    </ul>

    <p><strong>Getting Started with MCP in iTaK:</strong></p>
    <ol>
        <li>Ensure your iTaK AI Stack is properly configured and running</li>
        <li>Access the MCP server at <code class="copyable" data-copy="http://localhost:8080">http://localhost:8080</code></li>
        <li>Connect your preferred AI models to the MCP server (see other tabs for specific instructions)</li>
        <li>Test the connection by requesting information that requires external tools</li>
    </ol>

    <p><strong>Example MCP Interaction:</strong></p>
    <pre><code class="copyable" data-copy="User: What were the major tech news headlines today?

AI: I'll need to check the latest news. Let me search for that information.

[AI uses web_search tool through MCP]

AI: Here are today's major tech headlines:
1. [Recent headline about a tech company]
2. [Recent headline about a product launch]
3. [Recent headline about industry trends]

Would you like me to provide more details on any of these stories?">User: What were the major tech news headlines today?

AI: I'll need to check the latest news. Let me search for that information.

[AI uses web_search tool through MCP]

AI: Here are today's major tech headlines:
1. [Recent headline about a tech company]
2. [Recent headline about a product launch]
3. [Recent headline about industry trends]

Would you like me to provide more details on any of these stories?</code></pre>

    <p><strong>MCP vs. Traditional AI:</strong></p>
    <table class="model-table">
        <tr>
            <th>Capability</th>
            <th>Traditional AI</th>
            <th>AI with MCP</th>
        </tr>
        <tr>
            <td>Information Access</td>
            <td>Limited to training data</td>
            <td>Can access real-time information</td>
        </tr>
        <tr>
            <td>Problem Solving</td>
            <td>Text-based reasoning only</td>
            <td>Can execute code and perform calculations</td>
        </tr>
        <tr>
            <td>Data Handling</td>
            <td>Cannot access or manipulate files</td>
            <td>Can read, write, and process data files</td>
        </tr>
        <tr>
            <td>System Integration</td>
            <td>Isolated from other systems</td>
            <td>Can interact with APIs and databases</td>
        </tr>
    </table>
</div>



                <!-- MCP + Ollama -->
<div class="integration-content" id="mcp-ollama">
    <h4>Connecting Ollama Models to MCP</h4>
    <p>Integrate your local Ollama models with Model Context Protocol to significantly enhance their capabilities and utility while maintaining privacy and control.</p>

    <div class="important-note">
        <h5>🚀 Why Connect Ollama to MCP?</h5>
        <p>Connecting Ollama models to MCP provides several key advantages:</p>
        <ul>
            <li>Run powerful AI models locally while accessing external tools and information</li>
            <li>Maintain privacy by keeping your data and model execution on your own hardware</li>
            <li>Overcome knowledge cutoff limitations by accessing current information</li>
            <li>Enable your local models to perform complex tasks like calculations and data analysis</li>
            <li>Create more capable AI assistants without relying on cloud-based services</li>
        </ul>
    </div>

    <p><strong>Setup Requirements:</strong></p>
    <ul>
        <li><strong>Ollama:</strong> Properly installed and running on your local machine</li>
        <li><strong>MCP Server:</strong> Running as part of your iTaK AI Stack</li>
        <li><strong>Compatible Models:</strong> Models that support function calling (e.g., Llama 3, Mistral, etc.)</li>
        <li><strong>Node.js:</strong> For running the connection script (optional)</li>
    </ul>

    <p><strong>Connection Methods:</strong></p>
    <table class="model-table">
        <tr>
            <th>Method</th>
            <th>Complexity</th>
            <th>Best For</th>
        </tr>
        <tr>
            <td>Direct API Integration</td>
            <td>Medium</td>
            <td>Developers building custom applications</td>
        </tr>
        <tr>
            <td>MCP Bridge Script</td>
            <td>Low</td>
            <td>Quick setup for testing and personal use</td>
        </tr>
        <tr>
            <td>WebUI Integration</td>
            <td>Low</td>
            <td>Users who prefer graphical interfaces</td>
        </tr>
        <tr>
            <td>Custom Prompt Engineering</td>
            <td>Medium</td>
            <td>Advanced users who want to fine-tune behavior</td>
        </tr>
    </table>

    <p><strong>Quick Start with MCP Bridge Script:</strong></p>
    <ol>
        <li>Create a new file named <code>ollama-mcp-bridge.js</code></li>
        <li>Copy the following code into the file:</li>
    </ol>

    <pre><code class="copyable" data-copy="const axios = require('axios');

// Configuration
const OLLAMA_URL = 'http://localhost:11434';
const MCP_URL = 'http://localhost:8080';
const DEFAULT_MODEL = 'llama3';

// Function to call Ollama
async function callOllama(prompt, model = DEFAULT_MODEL) {
  try {
    const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
      model,
      prompt,
      stream: false
    });
    return response.data.response;
  } catch (error) {
    console.error('Error calling Ollama:', error.message);
    return 'Error: Failed to get response from Ollama';
  }
}

// Function to call MCP tool
async function callMCPTool(query, tool) {
  try {
    const response = await axios.post(`${MCP_URL}/v1/tools`, {
      query,
      tools: [tool]
    });
    return response.data.result;
  } catch (error) {
    console.error('Error calling MCP tool:', error.message);
    return { error: 'Failed to call MCP tool' };
  }
}

// Main function to process user input
async function processUserInput(userInput) {
  // First, ask Ollama if it needs to use a tool
  const systemPrompt = `You are a helpful assistant with access to external tools. 
When you need information you don't have, explicitly state which tool you need:
- web_search: For finding information on the internet
- code_interpreter: For running code or calculations
- file_system: For reading or writing files

Format your response as: "I need to use [tool_name] to [reason]"`;

  const initialPrompt = `${systemPrompt}\n\nUser: ${userInput}\nAssistant:`;
  
  const ollamaResponse = await callOllama(initialPrompt);
  console.log('Initial response:', ollamaResponse);
  
  // Check if Ollama wants to use a tool
  const toolMatch = ollamaResponse.match(/I need to use (\w+) to/i);
  
  if (toolMatch && toolMatch[1]) {
    const toolName = toolMatch[1].toLowerCase();
    console.log(`Using tool: ${toolName}`);
    
    // Call the appropriate MCP tool
    const toolResult = await callMCPTool(userInput, toolName);
    
    // Send the tool result back to Ollama for final response
    const finalPrompt = `${systemPrompt}\n\nUser: ${userInput}\n\nI used the ${toolName} tool and got this result: ${JSON.stringify(toolResult)}\n\nBased on this information, please provide a helpful response.\nAssistant:`;
    
    const finalResponse = await callOllama(finalPrompt);
    return finalResponse;
  }
  
  return ollamaResponse;
}

// Example usage
async function main() {
  const userQuestion = process.argv[2] || 'What is the current weather in New York?';
  console.log(`User question: ${userQuestion}`);
  
  const response = await processUserInput(userQuestion);
  console.log('\nFinal response:');
  console.log(response);
}

main().catch(console.error);">const axios = require('axios');

// Configuration
const OLLAMA_URL = 'http://localhost:11434';
const MCP_URL = 'http://localhost:8080';
const DEFAULT_MODEL = 'llama3';

// Function to call Ollama
async function callOllama(prompt, model = DEFAULT_MODEL) {
  try {
    const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
      model,
      prompt,
      stream: false
    });
    return response.data.response;
  } catch (error) {
    console.error('Error calling Ollama:', error.message);
    return 'Error: Failed to get response from Ollama';
  }
}

// Function to call MCP tool
async function callMCPTool(query, tool) {
  try {
    const response = await axios.post(`${MCP_URL}/v1/tools`, {
      query,
      tools: [tool]
    });
    return response.data.result;
  } catch (error) {
    console.error('Error calling MCP tool:', error.message);
    return { error: 'Failed to call MCP tool' };
  }
}

// Main function to process user input
async function processUserInput(userInput) {
  // First, ask Ollama if it needs to use a tool
  const systemPrompt = `You are a helpful assistant with access to external tools. 
When you need information you don't have, explicitly state which tool you need:
- web_search: For finding information on the internet
- code_interpreter: For running code or calculations
- file_system: For reading or writing files

Format your response as: "I need to use [tool_name] to [reason]"`;

  const initialPrompt = `${systemPrompt}\n\nUser: ${userInput}\nAssistant:`;
  
  const ollamaResponse = await callOllama(initialPrompt);
  console.log('Initial response:', ollamaResponse);
  
  // Check if Ollama wants to use a tool
  const toolMatch = ollamaResponse.match(/I need to use (\w+) to/i);
  
  if (toolMatch && toolMatch[1]) {
    const toolName = toolMatch[1].toLowerCase();
    console.log(`Using tool: ${toolName}`);
    
    // Call the appropriate MCP tool
    const toolResult = await callMCPTool(userInput, toolName);
    
    // Send the tool result back to Ollama for final response
    const finalPrompt = `${systemPrompt}\n\nUser: ${userInput}\n\nI used the ${toolName} tool and got this result: ${JSON.stringify(toolResult)}\n\nBased on this information, please provide a helpful response.\nAssistant:`;
    
    const finalResponse = await callOllama(finalPrompt);
    return finalResponse;
  }
  
  return ollamaResponse;
}

// Example usage
async function main() {
  const userQuestion = process.argv[2] || 'What is the current weather in New York?';
  console.log(`User question: ${userQuestion}`);
  
  const response = await processUserInput(userQuestion);
  console.log('\nFinal response:');
  console.log(response);
}

main().catch(console.error);</code></pre>

    <p><strong>Running the Bridge:</strong></p>
    <ol>
        <li>Install required dependencies: <code class="copyable" data-copy="npm install axios">npm install axios</code></li>
        <li>Run the script with a question: <code class="copyable" data-copy="node ollama-mcp-bridge.js 'What is the current price of Bitcoin?'">node ollama-mcp-bridge.js "What is the current price of Bitcoin?"</code></li>
        <li>The script will automatically:
            <ul>
                <li>Ask Ollama if it needs to use a tool</li>
                <li>Call the appropriate MCP tool if needed</li>
                <li>Return the final response with the information</li>
            </ul>
        </li>
    </ol>

    <p><strong>Advanced Configuration Options:</strong></p>
    <ul>
        <li><strong>Model Selection:</strong> Change the DEFAULT_MODEL variable to use different Ollama models</li>
        <li><strong>System Prompt:</strong> Customize the systemPrompt variable to change how the model uses tools</li>
        <li><strong>Tool Selection:</strong> Modify the available tools list to limit or expand capabilities</li>
        <li><strong>Response Format:</strong> Adjust the prompt templates to change how responses are structured</li>
    </ul>

    <p><strong>Example Interaction:</strong></p>
    <pre><code class="copyable" data-copy="$ node ollama-mcp-bridge.js 'What were the key announcements at the latest Apple event?'

User question: What were the key announcements at the latest Apple event?
Initial response: I need to use web_search to find information about the latest Apple event and its key announcements.
Using tool: web_search

Final response:
Based on the web search results, the latest Apple event (as of my last update) included several key announcements:

1. New iPhone models with improved cameras, processors, and battery life
2. Updates to the Apple Watch lineup with new health features
3. iOS and macOS updates with new productivity and privacy features
4. Enhancements to Apple services like Apple TV+ and Apple Music
5. New MacBook models featuring the latest Apple Silicon chips

The event also included demonstrations of new features and capabilities across Apple's ecosystem. For the most specific and current information, you might want to check Apple's official website or recent tech news sources.">$ node ollama-mcp-bridge.js 'What were the key announcements at the latest Apple event?'

User question: What were the key announcements at the latest Apple event?
Initial response: I need to use web_search to find information about the latest Apple event and its key announcements.
Using tool: web_search

Final response:
Based on the web search results, the latest Apple event (as of my last update) included several key announcements:

1. New iPhone models with improved cameras, processors, and battery life
2. Updates to the Apple Watch lineup with new health features
3. iOS and macOS updates with new productivity and privacy features
4. Enhancements to Apple services like Apple TV+ and Apple Music
5. New MacBook models featuring the latest Apple Silicon chips

The event also included demonstrations of new features and capabilities across Apple's ecosystem. For the most specific and current information, you might want to check Apple's official website or recent tech news sources.</code></pre>

    <p><strong>Troubleshooting Common Issues:</strong></p>
    <table class="model-table">
        <tr>
            <th>Issue</th>
            <th>Possible Cause</th>
            <th>Solution</th>
        </tr>
        <tr>
            <td>Connection refused errors</td>
            <td>Ollama or MCP server not running</td>
            <td>Verify both services are active and listening on the correct ports</td>
        </tr>
        <tr>
            <td>Model not responding correctly</td>
            <td>Incompatible model or incorrect prompt format</td>
            <td>Try a different model or adjust the system prompt</td>
        </tr>
        <tr>
            <td>Tool not being called</td>
            <td>Model not recognizing when to use tools</td>
            <td>Make the system prompt more explicit about tool usage</td>
        </tr>
        <tr>
            <td>Slow responses</td>
            <td>Large model running on limited hardware</td>
            <td>Try a smaller model or optimize parameters</td>
        </tr>
    </table>
</div>



               <!-- Rewritten MCP + Hugging Face -->
<div class="integration-content" id="mcp-huggingface">
    <h4>Integrating MCP with Hugging Face Models</h4>
    <p>You can combine the power of Hugging Face models with the utility of MCP to create smart systems capable of retrieving real-time data, executing code, and accessing external resources.</p>

    <div class="important-note">
        <h5>🤝 Why Use Hugging Face with MCP?</h5>
        <ul>
            <li>Leverage open-source models for text, vision, and code tasks</li>
            <li>Enable models to interact with external tools via MCP</li>
            <li>Expand model functionality without cloud dependency</li>
            <li>Great for local deployments and privacy-conscious applications</li>
        </ul>
    </div>

    <p><strong>Basic Setup:</strong></p>
    <ol>
        <li>Ensure Hugging Face UI/API is running locally (default: <code>http://localhost:3000</code>)</li>
        <li>Ensure MCP server is running (default: <code>http://localhost:8080</code>)</li>
        <li>Use a Node.js or Python script to integrate them</li>
    </ol>

    <p><strong>Example Prompt Flow:</strong></p>
    <ol>
        <li>Send user input to Hugging Face model</li>
        <li>Ask model if it needs a tool</li>
        <li>If yes, call MCP tool API</li>
        <li>Feed results back to model for response</li>
    </ol>

    <p><strong>Example System Prompt:</strong></p>
    <pre><code class="copyable" data-copy="You are a helpful assistant. If you need information, indicate the tool to use: web_search, code_interpreter, file_system. Format: 'I need to use [tool_name] to [reason]'">
You are a helpful assistant. If you need information, indicate the tool to use: web_search, code_interpreter, file_system. Format: 'I need to use [tool_name] to [reason]'
</code></pre>

    <p><strong>Simple API Call Sequence:</strong></p>
    <pre><code class="copyable" data-copy="1. Call Hugging Face with system prompt + user input
2. If tool needed, call MCP tool API
3. Append MCP result and re-call Hugging Face for final response">
1. Call Hugging Face with system prompt + user input
2. If tool needed, call MCP tool API
3. Append MCP result and re-call Hugging Face for final response
</code></pre>
</div>

              <!-- Rewritten Building MCP Apps -->
<div class="integration-content" id="mcp-apps">
    <h4>Building Custom Apps with MCP</h4>
    <p>You can create intelligent, tool-using apps using MCP as the central controller for external tool access.</p>

    <div class="important-note">
        <h5>🛠️ App Architecture</h5>
        <ul>
            <li>Frontend (React, Vue, etc.) handles user interaction</li>
            <li>Backend (Node.js, Python) processes prompts and routes through MCP</li>
            <li>MCP provides access to real-world tools (search, compute, files, APIs)</li>
        </ul>
    </div>

    <p><strong>Steps to Build:</strong></p>
    <ol>
        <li>Set up MCP and ensure it’s accessible at <code>http://localhost:8080</code></li>
        <li>Build an input form on the frontend</li>
        <li>Send user queries to your backend endpoint</li>
        <li>Let backend interact with a model (Ollama, Hugging Face)</li>
        <li>If tool is needed, call MCP and inject results back into the model</li>
        <li>Display model’s final response in the frontend</li>
    </ol>

    <p><strong>Helpful MCP Tool API:</strong></p>
    <pre><code class="copyable" data-copy='POST /v1/tools
{
  "query": "What’s the current price of ETH?",
  "tools": ["web_search"]
}'>
POST /v1/tools
{
  "query": "What’s the current price of ETH?",
  "tools": ["web_search"]
}
</code></pre>

    <p><strong>Tips:</strong></p>
    <ul>
        <li>Use React for dynamic interfaces with live updates</li>
        <li>Call MCP from backend only, not frontend</li>
        <li>Use consistent system prompts for expected behavior</li>
    </ul>

    <p><strong>Example Use Case:</strong></p>
    <pre><code class="copyable" data-copy="User: Show me top crypto trends today
Model: I need to use web_search to find current trends
MCP Result: [List of trends]
Final Answer: Here are today’s top crypto trends...">
User: Show me top crypto trends today
Model: I need to use web_search to find current trends
MCP Result: [List of trends]
Final Answer: Here are today’s top crypto trends...
</code></pre>
</div>


                
                <!-- MCP + iTaK Services -->
<div class="integration-content" id="mcp-itak">
    <h4>Integrating MCP with iTaK AI Stack Services</h4>
    <p>Learn how to connect Model Context Protocol with other services in your iTaK AI Stack to create powerful, interconnected AI workflows and applications.</p>

    <div class="important-note">
        <h5>🔄 Why Integrate MCP with AI Services?</h5>
        <p>Connecting MCP with other iTaK services creates a powerful ecosystem:</p>
        <ul>
            <li>Combine the strengths of multiple AI services for more capable applications</li>
            <li>Create seamless workflows that leverage different specialized tools</li>
            <li>Enable AI models to interact with your entire technology stack</li>
            <li>Build complex applications with minimal coding through service integration</li>
            <li>Maintain a consistent user experience across different AI capabilities</li>
        </ul>
    </div>

    <p><strong>Key iTaK Services for MCP Integration:</strong></p>
    <ul>
        <li><strong>Ollama:</strong> Local AI model execution for privacy and control</li>
        <li><strong>Flowise:</strong> Visual AI workflow builder for complex processes</li>
        <li><strong>n8n:</strong> Automation platform for connecting services and APIs</li>
        <li><strong>Hugging Face:</strong> Access to thousands of open-source AI models</li>
        <li><strong>SearXNG:</strong> Private search engine for information retrieval</li>
        <li><strong>Playwright:</strong> Web automation and scraping capabilities</li>
    </ul>

    <p><strong>Integration Patterns:</strong></p>
    <table class="model-table">
        <tr>
            <th>Pattern</th>
            <th>Services Involved</th>
            <th>Best For</th>
        </tr>
        <tr>
            <td>AI-Powered Automation</td>
            <td>MCP + n8n + Ollama</td>
            <td>Automated workflows with intelligent decision-making</td>
        </tr>
        <tr>
            <td>Enhanced Knowledge Base</td>
            <td>MCP + SearXNG + Flowise</td>
            <td>Creating AI assistants with access to current information</td>
        </tr>
        <tr>
            <td>Multi-Model Orchestration</td>
            <td>MCP + Ollama + Hugging Face</td>
            <td>Leveraging different models for specialized tasks</td>
        </tr>
        <tr>
            <td>Data Processing Pipeline</td>
            <td>MCP + Playwright + n8n</td>
            <td>Collecting, processing, and analyzing web data</td>
        </tr>
    </table>

    <p><strong>MCP + n8n Integration:</strong></p>
    <p>Use n8n to create workflows that leverage MCP's capabilities:</p>
    <ol>
        <li>In n8n, create a new workflow</li>
        <li>Add an HTTP Request node to call the MCP server:</li>
        <ul>
            <li>URL: <code class="copyable" data-copy="http://host.docker.internal:8080/v1/tools">http://host.docker.internal:8080/v1/tools</code></li>
            <li>Method: POST</li>
            <li>Body: JSON with query and tools parameters</li>
        </ul>
        <li>Example JSON body:</li>
    </ol>
    <pre><code class="copyable" data-copy='{
  "query": "{{$node["Previous Node"].data.userQuery}}",
  "tools": ["web_search"]
}'>{
  "query": "{{$node["Previous Node"].data.userQuery}}",
  "tools": ["web_search"]
}</code></pre>
    <li>Connect the MCP node to subsequent nodes to process the results</li>
    <li>You can use n8n's Webhook node to create an API endpoint that triggers this workflow</li>

    <p><strong>MCP + Flowise Integration:</strong></p>
    <p>Incorporate MCP capabilities into your Flowise chatflows:</p>
    <ol>
        <li>In Flowise, create a new chatflow</li>
        <li>Add an API Tool node:</li>
        <ul>
            <li>URL: <code class="copyable" data-copy="http://host.docker.internal:8080/v1/tools">http://host.docker.internal:8080/v1/tools</code></li>
            <li>Method: POST</li>
            <li>Request Body: JSON with query and tools</li>
        </ul>
        <li>Connect the API Tool to your LLM node</li>
        <li>In the LLM node's system prompt, instruct the model to use the API Tool results</li>
    </ol>

    <p><strong>MCP + Ollama + Hugging Face Integration:</strong></p>
    <p>Create a system that leverages multiple model types for different tasks:</p>
    <pre><code class="copyable" data-copy="const express = require('express');
const axios = require('axios');
const app = express();
app.use(express.json());

// Configuration
const OLLAMA_URL = 'http://localhost:11434';
const HF_URL = 'http://localhost:3000';
const MCP_URL = 'http://localhost:8080';

// Function to determine which model to use based on task
function selectModel(task) {
  const taskTypes = {
    'general': { service: 'ollama', model: 'llama3' },
    'code': { service: 'ollama', model: 'codellama' },
    'vision': { service: 'huggingface', model: 'llava' },
    'specialized': { service: 'huggingface', model: 'mistral' }
  };
  
  return taskTypes[task] || taskTypes['general'];
}

// Function to call Ollama
async function callOllama(prompt, model = 'llama3') {
  const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
    model,
    prompt,
    stream: false
  });
  return response.data.response;
}

// Function to call Hugging Face
async function callHuggingFace(prompt, model) {
  const response = await axios.post(`${HF_URL}/api/generate`, {
    inputs: prompt,
    model: model,
    parameters: { max_length: 500 }
  });
  return response.data.generated_text;
}

// Function to call MCP tool
async function callMCPTool(query, tool) {
  const response = await axios.post(`${MCP_URL}/v1/tools`, {
    query,
    tools: [tool]
  });
  return response.data;
}

// Main API endpoint
app.post('/api/assistant', async (req, res) => {
  try {
    const { query, task = 'general' } = req.body;
    
    // Select appropriate model
    const modelInfo = selectModel(task);
    console.log(`Using ${modelInfo.service} with model ${modelInfo.model} for task: ${task}`);
    
    // First, ask the model if it needs to use a tool
    const systemPrompt = `You are a helpful assistant with access to external tools.
When you need information you don't have, explicitly state which tool you need:
- web_search: For finding information on the internet
- code_interpreter: For running code or calculations
- file_system: For reading or writing files

Format your response as: "I need to use [tool_name] to [reason]"`;

    const initialPrompt = `${systemPrompt}\n\nUser: ${query}\nAssistant:`;
    
    // Get initial response from selected model
    let initialResponse;
    if (modelInfo.service === 'ollama') {
      initialResponse = await callOllama(initialPrompt, modelInfo.model);
    } else {
      initialResponse = await callHuggingFace(initialPrompt, modelInfo.model);
    }
    
    // Check if model wants to use a tool
    const toolMatch = initialResponse.match(/I need to use (\w+) to/i);
    
    if (toolMatch && toolMatch[1]) {
      const toolName = toolMatch[1].toLowerCase();
      console.log(`Using tool: ${toolName}`);
      
      // Call the MCP tool
      const toolResult = await callMCPTool(query, toolName);
      
      // Get final response with tool results
      const finalPrompt = `${systemPrompt}\n\nUser: ${query}\n\nI used the ${toolName} tool and got this result: ${JSON.stringify(toolResult)}\n\nBased on this information, please provide a helpful response.\nAssistant:`;
      
      let finalResponse;
      if (modelInfo.service === 'ollama') {
        finalResponse = await callOllama(finalPrompt, modelInfo.model);
      } else {
        finalResponse = await callHuggingFace(finalPrompt, modelInfo.model);
      }
      
      res.json({
        response: finalResponse,
        model_used: `${modelInfo.service}/${modelInfo.model}`,
        tool_used: toolName
      });
    } else {
      // No tool needed
      res.json({
        response: initialResponse,
        model_used: `${modelInfo.service}/${modelInfo.model}`,
        tool_used: null
      });
    }
  } catch (error) {
    console.error('Error:', error);
    res.status(500).json({ error: error.message });
  }
});

const PORT = process.env.PORT || 3030;
app.listen(PORT, () => {
  console.log(`Multi-model MCP server running on port ${PORT}`);
});">const express = require('express');
const axios = require('axios');
const app = express();
app.use(express.json());

// Configuration
const OLLAMA_URL = 'http://localhost:11434';
const HF_URL = 'http://localhost:3000';
const MCP_URL = 'http://localhost:8080';

// Function to determine which model to use based on task
function selectModel(task) {
  const taskTypes = {
    'general': { service: 'ollama', model: 'llama3' },
    'code': { service: 'ollama', model: 'codellama' },
    'vision': { service: 'huggingface', model: 'llava' },
    'specialized': { service: 'huggingface', model: 'mistral' }
  };
  
  return taskTypes[task] || taskTypes['general'];
}

// Function to call Ollama
async function callOllama(prompt, model = 'llama3') {
  const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
    model,
    prompt,
    stream: false
  });
  return response.data.response;
}

// Function to call Hugging Face
async function callHuggingFace(prompt, model) {
  const response = await axios.post(`${HF_URL}/api/generate`, {
    inputs: prompt,
    model: model,
    parameters: { max_length: 500 }
  });
  return response.data.generated_text;
}

// Function to call MCP tool
async function callMCPTool(query, tool) {
  const response = await axios.post(`${MCP_URL}/v1/tools`, {
    query,
    tools: [tool]
  });
  return response.data;
}

// Main API endpoint
app.post('/api/assistant', async (req, res) => {
  try {
    const { query, task = 'general' } = req.body;
    
    // Select appropriate model
    const modelInfo = selectModel(task);
    console.log(`Using ${modelInfo.service} with model ${modelInfo.model} for task: ${task}`);
    
    // First, ask the model if it needs to use a tool
    const systemPrompt = `You are a helpful assistant with access to external tools.
When you need information you don't have, explicitly state which tool you need:
- web_search: For finding information on the internet
- code_interpreter: For running code or calculations
- file_system: For reading or writing files

Format your response as: "I need to use [tool_name] to [reason]"`;

    const initialPrompt = `${systemPrompt}\n\nUser: ${query}\nAssistant:`;
    
    // Get initial response from selected model
    let initialResponse;
    if (modelInfo.service === 'ollama') {
      initialResponse = await callOllama(initialPrompt, modelInfo.model);
    } else {
      initialResponse = await callHuggingFace(initialPrompt, modelInfo.model);
    }
    
    // Check if model wants to use a tool
    const toolMatch = initialResponse.match(/I need to use (\w+) to/i);
    
    if (toolMatch && toolMatch[1]) {
      const toolName = toolMatch[1].toLowerCase();
      console.log(`Using tool: ${toolName}`);
      
      // Call the MCP tool
      const toolResult = await callMCPTool(query, toolName);
      
      // Get final response with tool results
      const finalPrompt = `${systemPrompt}\n\nUser: ${query}\n\nI used the ${toolName} tool and got this result: ${JSON.stringify(toolResult)}\n\nBased on this information, please provide a helpful response.\nAssistant:`;
      
      let finalResponse;
      if (modelInfo.service === 'ollama') {
        finalResponse = await callOllama(finalPrompt, modelInfo.model);
      } else {
        finalResponse = await callHuggingFace(finalPrompt, modelInfo.model);
      }
      
      res.json({
        response: finalResponse,
        model_used: `${modelInfo.service}/${modelInfo.model}`,
        tool_used: toolName
      });
    } else {
      // No tool needed
      res.json({
        response: initialResponse,
        model_used: `${modelInfo.service}/${modelInfo.model}`,
        tool_used: null
      });
    }
  } catch (error) {
    console.error('Error:', error);
    res.status(500).json({ error: error.message });
  }
});

const PORT = process.env.PORT || 3030;
app.listen(PORT, () => {
  console.log(`Multi-model MCP server running on port ${PORT}`);
});</code></pre>

    <p><strong>MCP + SearXNG + Playwright Integration:</strong></p>
    <p>Create an enhanced web research assistant that combines search and web scraping:</p>
    <ol>
        <li>Set up a custom MCP tool that first searches with SearXNG</li>
        <li>For each search result, use Playwright to extract detailed information</li>
        <li>Combine and summarize the information for the AI model</li>
    </ol>
    <pre><code class="copyable" data-copy="// Enhanced web research tool
const express = require('express');
const axios = require('axios');
const { chromium } = require('playwright');
const router = express.Router();

// Configuration
const SEARXNG_URL = 'http://localhost:8080';
const MAX_RESULTS = 3;
const MAX_DEPTH = 2;

// Function to search with SearXNG
async function searchWithSearXNG(query) {
  try {
    const response = await axios.get(`${SEARXNG_URL}/search`, {
      params: {
        q: query,
        format: 'json',
        language: 'en'
      }
    });
    return response.data.results || [];
  } catch (error) {
    console.error('SearXNG search error:', error.message);
    return [];
  }
}

// Function to scrape content with Playwright
async function scrapeWithPlaywright(url, selectors = ['h1', 'h2', 'h3', 'p']) {
  const browser = await chromium.launch();
  try {
    const page = await browser.newPage();
    await page.goto(url, { timeout: 30000, waitUntil: 'domcontentloaded' });
    
    // Extract content based on selectors
    const content = await page.evaluate((selectors) => {
      const elements = document.querySelectorAll(selectors.join(', '));
      return Array.from(elements).map(el => ({
        tag: el.tagName.toLowerCase(),
        text: el.innerText.trim()
      }))
      .filter(item => item.text.length > 0)
      .slice(0, 50); // Limit to first 50 elements
    }, selectors);
    
    // Format the content
    const formattedContent = content.map(item => {
      if (item.tag.startsWith('h')) {
        return `## ${item.text}`;
      }
      return item.text;
    }).join('\n\n');
    
    return {
      url,
      title: await page.title(),
      content: formattedContent
    };
  } catch (error) {
    console.error(`Error scraping ${url}:`, error.message);
    return {
      url,
      title: 'Error',
      content: `Failed to scrape content: ${error.message}`
    };
  } finally {
    await browser.close();
  }
}

// Enhanced web research endpoint
router.post('/enhanced-web-research', async (req, res) => {
  try {
    const { query } = req.body;
    
    // Step 1: Search with SearXNG
    console.log(`Searching for: ${query}`);
    const searchResults = await searchWithSearXNG(query);
    const topResults = searchResults.slice(0, MAX_RESULTS);
    
    if (topResults.length === 0) {
      return res.json({
        query,
        results: [],
        summary: 'No search results found.'
      });
    }
    
    // Step 2: Scrape content from top results
    console.log(`Scraping ${topResults.length} results...`);
    const scrapingPromises = topResults.map(result => scrapeWithPlaywright(result.url));
    const scrapedContents = await Promise.all(scrapingPromises);
    
    // Step 3: Format the results
    const formattedResults = scrapedContents.map((content, index) => ({
      rank: index + 1,
      title: content.title,
      url: content.url,
      snippet: content.content.substring(0, 300) + '...',
      full_content: content.content
    }));
    
    // Return the enhanced research results
    res.json({
      query,
      results: formattedResults,
      summary: `Found and analyzed ${formattedResults.length} relevant sources.`
    });
    
  } catch (error) {
    console.error('Enhanced web research error:', error);
    res.status(500).json({ error: error.message });
  }
});

module.exports = router;">// Enhanced web research tool
const express = require('express');
const axios = require('axios');
const { chromium } = require('playwright');
const router = express.Router();

// Configuration
const SEARXNG_URL = 'http://localhost:8080';
const MAX_RESULTS = 3;
const MAX_DEPTH = 2;

// Function to search with SearXNG
async function searchWithSearXNG(query) {
  try {
    const response = await axios.get(`${SEARXNG_URL}/search`, {
      params: {
        q: query,
        format: 'json',
        language: 'en'
      }
    });
    return response.data.results || [];
  } catch (error) {
    console.error('SearXNG search error:', error.message);
    return [];
  }
}

// Function to scrape content with Playwright
async function scrapeWithPlaywright(url, selectors = ['h1', 'h2', 'h3', 'p']) {
  const browser = await chromium.launch();
  try {
    const page = await browser.newPage();
    await page.goto(url, { timeout: 30000, waitUntil: 'domcontentloaded' });
    
    // Extract content based on selectors
    const content = await page.evaluate((selectors) => {
      const elements = document.querySelectorAll(selectors.join(', '));
      return Array.from(elements).map(el => ({
        tag: el.tagName.toLowerCase(),
        text: el.innerText.trim()
      }))
      .filter(item => item.text.length > 0)
      .slice(0, 50); // Limit to first 50 elements
    }, selectors);
    
    // Format the content
    const formattedContent = content.map(item => {
      if (item.tag.startsWith('h')) {
        return `## ${item.text}`;
      }
      return item.text;
    }).join('\n\n');
    
    return {
      url,
      title: await page.title(),
      content: formattedContent
    };
  } catch (error) {
    console.error(`Error scraping ${url}:`, error.message);
    return {
      url,
      title: 'Error',
      content: `Failed to scrape content: ${error.message}`
    };
  } finally {
    await browser.close();
  }
}

// Enhanced web research endpoint
router.post('/enhanced-web-research', async (req, res) => {
  try {
    const { query } = req.body;
    
    // Step 1: Search with SearXNG
    console.log(`Searching for: ${query}`);
    const searchResults = await searchWithSearXNG(query);
    const topResults = searchResults.slice(0, MAX_RESULTS);
    
    if (topResults.length === 0) {
      return res.json({
        query,
        results: [],
        summary: 'No search results found.'
      });
    }
    
    // Step 2: Scrape content from top results
    console.log(`Scraping ${topResults.length} results...`);
    const scrapingPromises = topResults.map(result => scrapeWithPlaywright(result.url));
    const scrapedContents = await Promise.all(scrapingPromises);
    
    // Step 3: Format the results
    const formattedResults = scrapedContents.map((content, index) => ({
      rank: index + 1,
      title: content.title,
      url: content.url,
      snippet: content.content.substring(0, 300) + '...',
      full_content: content.content
    }));
    
    // Return the enhanced research results
    res.json({
      query,
      results: formattedResults,
      summary: `Found and analyzed ${formattedResults.length} relevant sources.`
    });
    
  } catch (error) {
    console.error('Enhanced web research error:', error);
    res.status(500).json({ error: error.message });
  }
});

module.exports = router;</code></pre>

    <p><strong>Example iTaK AI Stack Workflows:</strong></p>
    <table class="model-table">
        <tr>
            <th>Workflow</th>
            <th>Services Used</th>
            <th>Description</th>
        </tr>
        <tr>
            <td>Automated Research Assistant</td>
            <td>MCP + SearXNG + Ollama + n8n</td>
            <td>Automatically researches topics, summarizes findings, and generates reports</td>
        </tr>
        <tr>
            <td>Content Creation Pipeline</td>
            <td>MCP + Hugging Face + Flowise</td>
            <td>Creates blog posts, social media content, and images based on prompts</td>
        </tr>
        <tr>
            <td>Data Analysis Workbench</td>
            <td>MCP + Ollama + n8n + Supabase</td>
            <td>Analyzes data from various sources, generates insights, and stores results</td>
        </tr>
        <tr>
            <td>Intelligent Automation System</td>
            <td>MCP + n8n + Ollama + Playwright</td>
            <td>Automates complex workflows with AI-powered decision making</td>
        </tr>
    </table>

    <p><strong>Best Practices for iTaK Service Integration:</strong></p>
    <ul>
        <li><strong>Use Docker Networking:</strong> Leverage host.docker.internal for service-to-service communication</li>
        <li><strong>Implement Caching:</strong> Cache results to reduce redundant API calls between services</li>
        <li><strong>Error Handling:</strong> Design robust error handling and fallback mechanisms</li>
        <li><strong>Monitoring:</strong> Set up logging and monitoring across all integrated services</li>
        <li><strong>Security:</strong> Implement proper authentication between services</li>
    </ul>

    <p><strong>Getting Started with iTaK Service Integration:</strong></p>
    <ol>
        <li>Ensure all required services are running in your iTaK AI Stack</li>
        <li>Verify network connectivity between services using simple API calls</li>
        <li>Start with basic integrations before building more complex workflows</li>
        <li>Test each integration point thoroughly before connecting multiple services</li>
        <li>Document your integration patterns for future reference and troubleshooting</li>
    </ol>
</div>


                <div class="integration-content active" id="n8n-flowise">
                <h4>Using n8n with Flowise</h4>
                <p>You can use n8n to trigger Flowise workflows and process the results:</p>
                <ol>
                    <li>In n8n, add an HTTP Request node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:3001/api/v1/prediction/[your-flow-id]">http://host.docker.internal:3001/api/v1/prediction/[your-flow-id]</code></li>
                    <li>Set Method to POST</li>
                    <li>In the Body tab, add your input JSON: <code class="copyable" data-copy='{"question":"Your question here"}'>{'"question":"Your question here"'}</code></li>
                    <li>Connect to subsequent nodes to process the response</li>
                </ol>
            </div>

            <div class="integration-content" id="n8n-ollama">
                <h4>Using n8n with Ollama</h4>
                <p>You can use n8n to generate text with Ollama models:</p>
                <ol>
                    <li>In n8n, add an HTTP Request node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:11434/api/generate">http://host.docker.internal:11434/api/generate</code></li>
                    <li>Set Method to POST</li>
                    <li>In the Body tab, add your input JSON: <code class="copyable" data-copy='{"model":"llama3","prompt":"Your prompt here"}'>{'"model":"llama3","prompt":"Your prompt here"'}</code></li>
                    <li>Connect to subsequent nodes to process the response</li>
                </ol>
            </div>

            <div class="integration-content" id="n8n-searxng">
                <h4>Using n8n with SearXNG</h4>
                <p>You can use n8n to perform web searches with SearXNG:</p>
                <ol>
                    <li>In n8n, add an HTTP Request node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:8080/search">http://host.docker.internal:8080/search</code></li>
                    <li>Set Method to GET</li>
                    <li>Add a query parameter with key 'q' and value containing your search query</li>
                    <li>Add a parameter 'format' with value 'json' to get results in JSON format</li>
                    <li>Connect to subsequent nodes to process the search results</li>
                </ol>
            </div>

            <div class="integration-content" id="flowise-n8n">
                <h4>Using Flowise with n8n</h4>
                <p>You can call n8n webhooks from Flowise:</p>
                <ol>
                    <li>In Flowise, add an API Tool node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:5678/webhook/[your-webhook-id]">http://host.docker.internal:5678/webhook/[your-webhook-id]</code></li>
                    <li>Configure the method and parameters as needed</li>
                    <li>Connect the API Tool to your chat flow</li>
                </ol>
            </div>

            <div class="integration-content" id="flowise-ollama">
                <h4>Using Flowise with Ollama</h4>
                <p>You can integrate Ollama models directly in Flowise:</p>
                <ol>
                    <li>In Flowise, add an Ollama node</li>
                    <li>Set the Base URL to <code class="copyable" data-copy="http://host.docker.internal:11434">http://host.docker.internal:11434</code></li>
                    <li>Select your model (e.g., llama3, mistral, etc.)</li>
                    <li>Configure the parameters as needed</li>
                    <li>Connect the Ollama node to your chat flow</li>
                </ol>
            </div>

            <div class="integration-content" id="n8n-youtube">
                <h4>Using n8n with YouTube Downloader</h4>
                <p>You can use n8n to download videos and manage the YouTube Downloader:</p>
                <ol>
                    <li>In n8n, add an HTTP Request node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:8998/api/">http://host.docker.internal:8998/api/</code></li>
                    <li>Set Method to POST</li>
                    <li>Add the endpoint for the action you want (e.g., /api/download)</li>
                    <li>Add the required parameters for the video URL and format</li>
                    <li>Connect to subsequent nodes to process the download status</li>
                </ol>
            </div>

            <div class="integration-content" id="n8n-playwright">
                <h4>Using n8n with Playwright Web Scraper</h4>
                <p>You can use n8n to trigger web scraping jobs:</p>
                <ol>
                    <li>In n8n, add an HTTP Request node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:3333/scrape">http://host.docker.internal:3333/scrape</code></li>
                    <li>Set Method to POST</li>
                    <li>In the Body tab, add your scraping configuration: <code class="copyable" data-copy='{"url":"https://example.com","selector":"h1, p"}'>{'"url":"https://example.com","selector":"h1, p"'}</code></li>
                    <li>Connect to subsequent nodes to process the scraped data</li>
                </ol>
            </div>

            <div class="integration-content" id="flowise-searxng">
                <h4>Using Flowise with SearXNG</h4>
                <p>You can integrate SearXNG search into your Flowise workflows:</p>
                <ol>
                    <li>In Flowise, add an API Tool node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:8080/search">http://host.docker.internal:8080/search</code></li>
                    <li>Set Method to GET</li>
                    <li>Add a query parameter with key 'q' and value from your chat input</li>
                    <li>Add a parameter 'format' with value 'json'</li>
                    <li>Connect the API Tool to your chat flow</li>
                </ol>
            </div>

            <div class="integration-content" id="flowise-youtube">
                <h4>Using Flowise with YouTube Downloader</h4>
                <p>You can integrate YouTube downloading into your Flowise workflows:</p>
                <ol>
                    <li>In Flowise, add an API Tool node</li>
                    <li>Set the URL to <code class="copyable" data-copy="http://host.docker.internal:8998/api/download">http://host.docker.internal:8998/api/download</code></li>
                    <li>Set Method to POST</li>
                    <li>Add the required parameters for the video URL from your chat input</li>
                    <li>Connect the API Tool to your chat flow to process the download response</li>
                </ol>
            </div>

            <div class="integration-content" id="n8n-discord">
                <h4>Using n8n with Discord</h4>
                <p>You can use n8n to create Discord bots and automate Discord interactions:</p>

                <div class="important-note">
                    <h5>⚠️ HTTPS Requirement for Discord</h5>
                    <p>Discord requires HTTPS for webhooks in production environments. For the iTaK AI Stack:</p>
                    <ul>
                        <li>Use the HTTPS URLs provided in each service card for external access</li>
                        <li>Ensure your domain is properly set up with SSL certificates</li>
                        <li>For local testing, you can use a service like <a href="https://ngrok.com/" target="_blank">ngrok</a> to create a secure tunnel</li>
                    </ul>
                </div>

                <p><strong>Setting up a Discord Webhook:</strong></p>
                <ol>
                    <li>In Discord, go to a channel's settings → Integrations → Webhooks → Create Webhook</li>
                    <li>Copy the webhook URL</li>
                    <li>In n8n, add a Discord node</li>
                    <li>Paste your webhook URL</li>
                    <li>Configure the message content and formatting</li>
                </ol>

                <p><strong>Creating a Discord Bot:</strong></p>
                <ol>
                    <li>Go to the <a href="https://discord.com/developers/applications" target="_blank">Discord Developer Portal</a></li>
                    <li>Create a New Application and add a Bot</li>
                    <li>Copy the Bot Token</li>
                    <li>In n8n, use the Discord Bot node</li>
                    <li>Configure with your Bot Token</li>
                    <li>Set up triggers for messages, reactions, etc.</li>
                </ol>

                <p><strong>Using HTTPS with Discord:</strong></p>
                <ol>
                    <li>Access n8n via its HTTPS URL: <code class="copyable" data-copy="https://n8n.your-domain.com">https://n8n.your-domain.com</code></li>
                    <li>Set up a webhook in n8n that Discord can call (must be HTTPS)</li>
                    <li>For local development, use ngrok: <code class="copyable" data-copy="ngrok http 5678">ngrok http 5678</code></li>
                    <li>Use the ngrok HTTPS URL in your Discord webhook configuration</li>
                </ol>

                <p><strong>Example Webhook JSON:</strong></p>
                <pre><code class="copyable" data-copy='{"content":"Message from AI Stack","embeds":[{"title":"Notification","description":"Your task has completed","color":5814783}]}'>{"content":"Message from AI Stack","embeds":[{"title":"Notification","description":"Your task has completed","color":5814783}]}</code></pre>
            </div>

            <div class="integration-content" id="flowise-discord">
                <h4>Using Flowise with Discord</h4>
                <p>You can integrate Flowise with Discord to create AI-powered chat bots:</p>

                <div class="important-note">
                    <h5>⚠️ HTTPS Requirement for Discord</h5>
                    <p>Discord requires HTTPS for webhooks and API interactions. For Flowise integration:</p>
                    <ul>
                        <li>Use the HTTPS URL for Flowise: <code class="copyable" data-copy="https://flowise.your-domain.com">https://flowise.your-domain.com</code></li>
                        <li>Ensure your domain has valid SSL certificates</li>
                        <li>For local development, use ngrok to expose Flowise securely: <code class="copyable" data-copy="ngrok http 3001">ngrok http 3001</code></li>
                    </ul>
                </div>

                <p><strong>Using Discord Webhooks in Flowise:</strong></p>
                <ol>
                    <li>In Discord, create a webhook as described in the n8n section</li>
                    <li>In Flowise, add an API Tool node</li>
                    <li>Set the URL to your Discord webhook URL</li>
                    <li>Set Method to POST</li>
                    <li>Configure the JSON body with your message content</li>
                    <li>Connect to your chat flow to send AI responses to Discord</li>
                </ol>

                <p><strong>Creating a Discord AI Chatbot:</strong></p>
                <ol>
                    <li>Create a Discord bot in the Developer Portal</li>
                    <li>Set up a server to host a Discord.js application</li>
                    <li>Use the Flowise API to process messages</li>
                    <li>For internal access: <code class="copyable" data-copy="http://host.docker.internal:3001/api/v1/prediction/[your-flow-id]">http://host.docker.internal:3001/api/v1/prediction/[your-flow-id]</code></li>
                    <li>For external access: <code class="copyable" data-copy="https://flowise.your-domain.com/api/v1/prediction/[your-flow-id]">https://flowise.your-domain.com/api/v1/prediction/[your-flow-id]</code></li>
                    <li>Send user messages to Flowise and return responses to Discord</li>
                </ol>

                <p><strong>Setting Up a Discord Bot with Flowise:</strong></p>
                <ol>
                    <li>Install Discord.js: <code class="copyable" data-copy="npm install discord.js axios">npm install discord.js axios</code></li>
                    <li>Create a bot script that listens for messages</li>
                    <li>When a message is received, send it to Flowise API</li>
                    <li>Return the Flowise response to the Discord channel</li>
                    <li>Host this bot on a server with HTTPS access to both Discord and Flowise</li>
                </ol>

                <p><strong>Example Discord Message JSON:</strong></p>
                <pre><code class="copyable" data-copy='{"content":"","embeds":[{"title":"AI Response","description":"{{flowiseResponse}}","color":3447003}]}'>{"content":"","embeds":[{"title":"AI Response","description":"{{flowiseResponse}}","color":3447003}]}</code></pre>
            </div>

            <div class="integration-content" id="n8n-import">
                <h4>Importing n8n Workflows</h4>
                <p>You can import pre-built workflows into n8n to quickly get started with automation:</p>

                <p><strong>Importing from JSON:</strong></p>
                <ol>
                    <li>In n8n, click on "Workflows" in the left sidebar</li>
                    <li>Click the "Import from File" button (or Import from URL)</li>
                    <li>Select your workflow JSON file or paste the URL</li>
                    <li>Review the workflow and click "Import"</li>
                    <li>The workflow will be added to your n8n instance</li>
                </ol>

                <p><strong>Finding Pre-built Workflows:</strong></p>
                <ul>
                    <li><a href="https://n8n.io/workflows" target="_blank">n8n.io Workflow Library</a> - Official collection of workflows</li>
                    <li><a href="https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/credentials" target="_blank">n8n GitHub Repository</a> - Examples and templates</li>
                    <li><a href="https://community.n8n.io/" target="_blank">n8n Community Forum</a> - User-shared workflows</li>
                </ul>

                <p><strong>Example Workflow JSON Structure:</strong></p>
                <pre><code class="copyable" data-copy='{"name":"Example Workflow","nodes":[],"connections":{},"active":false,"settings":{},"id":"example"}'>{"name":"Example Workflow","nodes":[],"connections":{},"active":false,"settings":{},"id":"example"}</code></pre>

                <p><strong>Sharing Your Workflows:</strong></p>
                <ol>
                    <li>Open the workflow you want to share</li>
                    <li>Click the "Export" button in the top-right menu</li>
                    <li>Choose "Export to File" or "Export to URL"</li>
                    <li>Save the JSON file or copy the URL to share with others</li>
                </ol>
            </div>

            <div class="integration-content" id="flowise-import">
                <h4>Importing Flowise Chatflows</h4>
                <p>You can import pre-built chatflows into Flowise to quickly set up AI conversations:</p>

                <p><strong>Importing from JSON:</strong></p>
                <ol>
                    <li>In Flowise, go to the Chatflows page</li>
                    <li>Click the "Import" button in the top-right corner</li>
                    <li>Select your chatflow JSON file or paste the JSON directly</li>
                    <li>Click "Import" to add the chatflow to your Flowise instance</li>
                    <li>The imported chatflow will appear in your list</li>
                </ol>

                <p><strong>Finding Pre-built Chatflows:</strong></p>
                <ul>
                    <li><a href="https://github.com/FlowiseAI/Flowise" target="_blank">Flowise GitHub Repository</a> - Official examples</li>
                    <li><a href="https://discord.gg/jbaHfsRVBW" target="_blank">Flowise Discord Community</a> - User-shared chatflows</li>
                    <li><a href="https://flowiseai.com/" target="_blank">Flowise Website</a> - Templates and guides</li>
                </ul>

                <p><strong>Exporting Your Chatflows:</strong></p>
                <ol>
                    <li>Open the Chatflows page in Flowise</li>
                    <li>Find the chatflow you want to export</li>
                    <li>Click the three dots menu and select "Export"</li>
                    <li>Save the JSON file to your computer</li>
                    <li>You can now share this file with others or use it as a backup</li>
                </ol>

                <p><strong>Troubleshooting Imports:</strong></p>
                <ul>
                    <li>Ensure all required components are installed in your Flowise instance</li>
                    <li>Check that API keys and credentials are properly configured</li>
                    <li>Verify that the JSON structure is valid and complete</li>
                    <li>For complex chatflows, import dependencies (like tools or agents) first</li>
                </ul>
                </div>

                <!-- New integration content for Ollama + Flowise -->
                <div class="integration-content" id="ollama-flowise">
                    <h4>Using Ollama with Flowise</h4>
                    <p>Create powerful AI applications by connecting Ollama's local LLMs to Flowise:</p>

                    <div class="important-note">
                        <h5>🚀 Why This Integration is Powerful</h5>
                        <p>Combining Ollama with Flowise lets you:</p>
                        <ul>
                            <li>Run AI workflows completely locally without sending data to external APIs</li>
                            <li>Create custom chatbots with your own knowledge base</li>
                            <li>Build complex AI applications with no-code visual interface</li>
                            <li>Save money by not using paid API services</li>
                        </ul>
                    </div>

                    <p><strong>Setting Up Ollama in Flowise:</strong></p>
                    <ol>
                        <li>In Flowise, create a new chatflow</li>
                        <li>Add a "Chat Models" node</li>
                        <li>Select "Ollama" as the model type</li>
                        <li>Set the Base URL to <code class="copyable" data-copy="http://host.docker.internal:11434">http://host.docker.internal:11434</code></li>
                        <li>Choose your model (e.g., llama3, mistral, etc.)</li>
                        <li>Connect to a "Chat" node to create your interface</li>
                    </ol>

                    <p><strong>Building a RAG System with Ollama and Flowise:</strong></p>
                    <ol>
                        <li>Add a "Document Loaders" node to load your documents</li>
                        <li>Connect to a "Text Splitters" node to chunk your content</li>
                        <li>Add a "Vector Store" node (e.g., Chroma) for embedding storage</li>
                        <li>Create a "Retrieval" node to fetch relevant information</li>
                        <li>Connect your Ollama model to process the retrieved information</li>
                        <li>Add a prompt template that instructs the model to use the retrieved context</li>
                    </ol>

                    <p><strong>Example Prompt Template:</strong></p>
                    <pre><code class="copyable" data-copy="You are a helpful assistant. Use the following context to answer the user's question.\n\nContext: {{context}}\n\nQuestion: {{question}}\n\nAnswer:">You are a helpful assistant. Use the following context to answer the user's question.\n\nContext: {{context}}\n\nQuestion: {{question}}\n\nAnswer:</code></pre>

                    <p><strong>Advanced Techniques:</strong></p>
                    <ul>
                        <li><strong>Chain Multiple Models:</strong> Use different Ollama models for different tasks in your workflow</li>
                        <li><strong>Implement Memory:</strong> Add a "Memory" node to maintain conversation history</li>
                        <li><strong>Custom Tools:</strong> Create tool nodes that let your AI perform specific actions</li>
                        <li><strong>API Integration:</strong> Expose your Flowise+Ollama chatflow as an API for other applications</li>
                    </ul>

                    <p><strong>Performance Tips:</strong></p>
                    <ul>
                        <li>Use smaller models (like mistral or tinyllama) for faster responses</li>
                        <li>Adjust the context window size based on your needs</li>
                        <li>Use efficient embedding models for better retrieval accuracy</li>
                        <li>Consider adding a caching mechanism for frequent queries</li>
                    </ul>
                </div>

                <!-- Hugging Face + Ollama Integration -->
                <div class="integration-content" id="huggingface-ollama">
                    <h4>Using Hugging Face Chat UI with Ollama</h4>
                    <p>Create a seamless chat experience by connecting the Hugging Face Chat UI to your local Ollama models:</p>

                    <div class="important-note">
                        <h5>🔥 Why This Integration Matters</h5>
                        <p>The Hugging Face Chat UI + Ollama combination gives you:</p>
                        <ul>
                            <li>A beautiful, modern chat interface for your local models</li>
                            <li>Complete privacy with all processing happening on your machine</li>
                            <li>The ability to switch between different models easily</li>
                            <li>Support for streaming responses and markdown formatting</li>
                        </ul>
                    </div>

                    <p><strong>Setting Up the Connection:</strong></p>
                    <ol>
                        <li>Make sure both Ollama and Hugging Face Chat UI services are running</li>
                        <li>Open the Hugging Face Chat UI at <code class="copyable" data-copy="http://localhost:3000">http://localhost:3000</code></li>
                        <li>Click on the model selector in the top-right corner</li>
                        <li>Select "Custom API" from the dropdown</li>
                        <li>Enter the Ollama API URL: <code class="copyable" data-copy="http://host.docker.internal:11434/api/generate">http://host.docker.internal:11434/api/generate</code></li>
                        <li>Specify the model name (e.g., "llama3", "mistral", etc.)</li>
                        <li>Configure any additional parameters like temperature or max tokens</li>
                        <li>Click "Save" to connect to your Ollama model</li>
                    </ol>

                    <p><strong>Customizing the Prompt Template:</strong></p>
                    <pre><code class="copyable" data-copy="{{ if messages }}
{{ for message in messages }}
{{ if message.role == 'user' }}
User: {{ message.content }}
{{ else if message.role == 'assistant' }}
Assistant: {{ message.content }}
{{ else if message.role == 'system' }}
System: {{ message.content }}
{{ end }}
{{ end }}
{{ end }}
Assistant:">{{ if messages }}
{{ for message in messages }}
{{ if message.role == 'user' }}
User: {{ message.content }}
{{ else if message.role == 'assistant' }}
Assistant: {{ message.content }}
{{ else if message.role == 'system' }}
System: {{ message.content }}
{{ end }}
{{ end }}
{{ end }}
Assistant:</code></pre>

                    <p><strong>Advanced Configuration:</strong></p>
                    <ul>
                        <li><strong>System Prompts:</strong> Add a system prompt to guide the model's behavior</li>
                        <li><strong>Temperature:</strong> Adjust from 0 (more deterministic) to 1 (more creative)</li>
                        <li><strong>Max Tokens:</strong> Limit the length of responses</li>
                        <li><strong>Top P/Top K:</strong> Fine-tune the sampling strategy for better outputs</li>
                    </ul>

                    <p><strong>Troubleshooting:</strong></p>
                    <ul>
                        <li><strong>Connection Issues:</strong> Ensure Ollama is running and the URL is correct</li>
                        <li><strong>Model Not Found:</strong> Verify the model is downloaded in Ollama</li>
                        <li><strong>Slow Responses:</strong> Consider using a smaller model or adjusting parameters</li>
                        <li><strong>Formatting Problems:</strong> Check your prompt template for proper syntax</li>
                    </ul>
                </div>

                <!-- SearXNG + Ollama Integration -->
                <div class="integration-content" id="searxng-ollama">
                    <h4>Combining SearXNG with Ollama for Research</h4>
                    <p>Create a powerful research assistant by connecting SearXNG's search capabilities with Ollama's language models:</p>

                    <div class="important-note">
                        <h5>🧠 Building a Research Companion</h5>
                        <p>This integration enables you to:</p>
                        <ul>
                            <li>Search the web privately through SearXNG</li>
                            <li>Process and analyze search results with local AI models</li>
                            <li>Create summaries, extractions, and insights from web content</li>
                            <li>Build a complete research workflow without relying on cloud services</li>
                        </ul>
                    </div>

                    <p><strong>Manual Research Workflow:</strong></p>
                    <ol>
                        <li>Perform a search in SearXNG at <code class="copyable" data-copy="http://localhost:8080">http://localhost:8080</code></li>
                        <li>Copy relevant search results or open pages of interest</li>
                        <li>Paste the content into a prompt for Ollama (via Chat UI or API)</li>
                        <li>Ask Ollama to summarize, analyze, or extract information</li>
                    </ol>

                    <p><strong>Automated Research with n8n:</strong></p>
                    <ol>
                        <li>In n8n, create a new workflow</li>
                        <li>Add an HTTP Request node to query SearXNG: <code class="copyable" data-copy="http://host.docker.internal:8080/search?q=your+query&format=json">http://host.docker.internal:8080/search?q=your+query&format=json</code></li>
                        <li>Add a Function node to extract and format the search results</li>
                        <li>Add another HTTP Request node to send to Ollama: <code class="copyable" data-copy="http://host.docker.internal:11434/api/generate">http://host.docker.internal:11434/api/generate</code></li>
                        <li>Configure the Ollama request with a prompt that includes the search results</li>
                    </ol>

                    <p><strong>Example Prompt for Research:</strong></p>
                    <pre><code class="copyable" data-copy="I'm researching the topic of [TOPIC]. Here are some search results from the web:

[SEARCH_RESULTS]

Please provide:
1. A comprehensive summary of the information
2. Key points and insights
3. Any contradictions or areas needing further research
4. Suggested next steps for my research">I'm researching the topic of [TOPIC]. Here are some search results from the web:

[SEARCH_RESULTS]

Please provide:
1. A comprehensive summary of the information
2. Key points and insights
3. Any contradictions or areas needing further research
4. Suggested next steps for my research</code></pre>

                    <p><strong>Customizing SearXNG for Better Results:</strong></p>
                    <ul>
                        <li>Configure preferred search engines in SearXNG settings</li>
                        <li>Adjust the number of results per page</li>
                        <li>Use search operators like site:, filetype:, etc.</li>
                        <li>Enable or disable specific categories of results</li>
                    </ul>
                </div>

                <!-- YouTube + Ollama Integration -->
                <div class="integration-content" id="youtube-ollama">
                    <h4>Analyzing YouTube Videos with Ollama</h4>
                    <p>Create a powerful video analysis system by combining YouTube Downloader with Ollama's language models:</p>

                    <div class="important-note">
                        <h5>🎥 Video Content Analysis</h5>
                        <p>This integration allows you to:</p>
                        <ul>
                            <li>Download and transcribe YouTube videos</li>
                            <li>Analyze video content with local AI models</li>
                            <li>Extract insights, summaries, and key points</li>
                            <li>Build a knowledge base from educational videos</li>
                        </ul>
                    </div>

                    <p><strong>Basic Workflow:</strong></p>
                    <ol>
                        <li>Download a YouTube video using the YouTube Downloader at <code class="copyable" data-copy="http://localhost:8998">http://localhost:8998</code></li>
                        <li>Extract the audio and transcribe it (using Whisper via Ollama or another transcription service)</li>
                        <li>Send the transcript to Ollama for analysis</li>
                        <li>Store or use the insights generated by the AI</li>
                    </ol>

                    <p><strong>Downloading Videos:</strong></p>
                    <ol>
                        <li>Open the YouTube Downloader interface</li>
                        <li>Paste a YouTube URL in the input field</li>
                        <li>Select your preferred format and quality</li>
                        <li>Click "Download" to save the video locally</li>
                    </ol>

                    <p><strong>Extracting Audio:</strong></p>
                    <pre><code class="copyable" data-copy="ffmpeg -i video.mp4 -q:a 0 -map a audio.mp3">ffmpeg -i video.mp4 -q:a 0 -map a audio.mp3</code></pre>

                    <p><strong>Transcribing with Whisper (via Ollama):</strong></p>
                    <ol>
                        <li>Make sure you have the Whisper model in Ollama: <code class="copyable" data-copy="ollama pull whisper">ollama pull whisper</code></li>
                        <li>Use the Ollama API to transcribe: <code class="copyable" data-copy="curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper">curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper</code></li>
                    </ol>

                    <p><strong>Example Prompt for Video Analysis:</strong></p>
                    <pre><code class="copyable" data-copy="Here is a transcript from a YouTube video about [TOPIC]:

[TRANSCRIPT]

Please provide:
1. A concise summary of the video content
2. Key points and main arguments presented
3. Any technical terms or concepts explained, with definitions
4. The overall perspective or bias of the presenter
5. Areas where the information might be incomplete or could be expanded">Here is a transcript from a YouTube video about [TOPIC]:

[TRANSCRIPT]

Please provide:
1. A concise summary of the video content
2. Key points and main arguments presented
3. Any technical terms or concepts explained, with definitions
4. The overall perspective or bias of the presenter
5. Areas where the information might be incomplete or could be expanded</code></pre>

                    <p><strong>Batch Processing Multiple Videos:</strong></p>
                    <ul>
                        <li>Create a playlist or collection of related videos</li>
                        <li>Process them sequentially using a script or n8n workflow</li>
                        <li>Combine the insights from multiple videos on the same topic</li>
                        <li>Use Ollama to synthesize information across videos</li>
                    </ul>
                </div>

                <!-- Playwright + Flowise Integration -->
                <div class="integration-content" id="playwright-flowise">
                    <h4>Building Web Scrapers with Playwright and Flowise</h4>
                    <p>Create intelligent web scraping workflows by combining Playwright's browser automation with Flowise's visual AI builder:</p>

                    <div class="important-note">
                        <h5>🔍 Advanced Web Data Collection</h5>
                        <p>This integration enables you to:</p>
                        <ul>
                            <li>Scrape complex websites with full browser rendering</li>
                            <li>Process and analyze web content with AI</li>
                            <li>Extract structured data from unstructured web pages</li>
                            <li>Build automated research and data collection systems</li>
                        </ul>
                    </div>

                    <p><strong>Basic Workflow:</strong></p>
                    <ol>
                        <li>Use Playwright to scrape content from websites</li>
                        <li>Send the scraped content to Flowise for processing</li>
                        <li>Use Flowise's AI capabilities to extract, analyze, and structure the data</li>
                        <li>Store or use the processed information</li>
                    </ol>

                    <p><strong>Setting Up Playwright API:</strong></p>
                    <ol>
                        <li>Access the Playwright Web Scraper at <code class="copyable" data-copy="http://localhost:3333">http://localhost:3333</code></li>
                        <li>Use the API endpoint: <code class="copyable" data-copy="http://host.docker.internal:3333/api/scrape">http://host.docker.internal:3333/api/scrape</code></li>
                        <li>Send a POST request with your scraping parameters</li>
                    </ol>

                    <p><strong>Example Scraping Request:</strong></p>
                    <pre><code class="copyable" data-copy='{
  "url": "https://example.com",
  "selectors": [
    {
      "name": "title",
      "selector": "h1",
      "type": "text"
    },
    {
      "name": "paragraphs",
      "selector": "p",
      "type": "text",
      "multiple": true
    },
    {
      "name": "links",
      "selector": "a",
      "type": "attribute",
      "attribute": "href",
      "multiple": true
    }
  ]
}'>{"url":"https://example.com","selectors":[{"name":"title","selector":"h1","type":"text"},{"name":"paragraphs","selector":"p","type":"text","multiple":true},{"name":"links","selector":"a","type":"attribute","attribute":"href","multiple":true}]}</code></pre>

                    <p><strong>Integrating with Flowise:</strong></p>
                    <ol>
                        <li>In Flowise, create a new chatflow</li>
                        <li>Add a "Tool" node and configure it as a custom tool</li>
                        <li>Set up the tool to make HTTP requests to the Playwright API</li>
                        <li>Connect the tool to an LLM node (like Ollama)</li>
                        <li>Create a prompt template that instructs the AI to analyze the scraped content</li>
                    </ol>

                    <p><strong>Example Tool Configuration in Flowise:</strong></p>
                    <ul>
                        <li><strong>Tool Name:</strong> WebScraper</li>
                        <li><strong>Description:</strong> Scrapes content from websites using Playwright</li>
                        <li><strong>Schema:</strong> <code class="copyable" data-copy='{"type":"object","properties":{"url":{"type":"string","description":"The URL to scrape"},"selectors":{"type":"array","items":{"type":"object","properties":{"name":{"type":"string"},"selector":{"type":"string"},"type":{"type":"string","enum":["text","html","attribute"]},"attribute":{"type":"string"},"multiple":{"type":"boolean"}}}}},"required":["url"]}'>{"type":"object","properties":{"url":{"type":"string","description":"The URL to scrape"},"selectors":{"type":"array","items":{"type":"object","properties":{"name":{"type":"string"},"selector":{"type":"string"},"type":{"type":"string","enum":["text","html","attribute"]},"attribute":{"type":"string"},"multiple":{"type":"boolean"}}}}},"required":["url"]}</code></li>
                        <li><strong>Function:</strong> Code that makes the API call to Playwright</li>
                    </ul>

                    <p><strong>Advanced Techniques:</strong></p>
                    <ul>
                        <li><strong>Pagination:</strong> Configure Playwright to navigate through multiple pages</li>
                        <li><strong>Authentication:</strong> Handle login forms and authenticated sessions</li>
                        <li><strong>Dynamic Content:</strong> Wait for JavaScript-rendered content to load</li>
                        <li><strong>Data Cleaning:</strong> Use Flowise to clean and structure messy web data</li>
                    </ul>
                </div>

                <!-- Portainer Management Integration -->
                <div class="integration-content" id="portainer-management">
                    <h4>Managing Your AI Stack with Portainer</h4>
                    <p>Use Portainer to efficiently manage, monitor, and troubleshoot all the services in your AI Stack:</p>

                    <div class="important-note">
                        <h5>📊 Container Management Made Easy</h5>
                        <p>Portainer helps you:</p>
                        <ul>
                            <li>Monitor resource usage across all services</li>
                            <li>Start, stop, and restart containers with a visual interface</li>
                            <li>View logs to troubleshoot issues</li>
                            <li>Manage container configurations without command line</li>
                        </ul>
                    </div>

                    <p><strong>Getting Started with Portainer:</strong></p>
                    <ol>
                        <li>Access Portainer at <code class="copyable" data-copy="http://localhost:9000">http://localhost:9000</code></li>
                        <li>Create an admin account on first login</li>
                        <li>Select the "local" Docker environment</li>
                        <li>You'll see a dashboard with all your running containers</li>
                    </ol>

                    <p><strong>Managing Containers:</strong></p>
                    <ul>
                        <li><strong>View Details:</strong> Click on a container to see its configuration, logs, and stats</li>
                        <li><strong>Start/Stop:</strong> Use the play/stop buttons to control container state</li>
                        <li><strong>Logs:</strong> View real-time logs to troubleshoot issues</li>
                        <li><strong>Console:</strong> Access a terminal inside the container for advanced troubleshooting</li>
                    </ul>

                    <p><strong>Monitoring Resource Usage:</strong></p>
                    <ol>
                        <li>Go to the "Containers" section in Portainer</li>
                        <li>You'll see CPU, memory, and network usage for each container</li>
                        <li>Click on a container and go to the "Stats" tab for detailed metrics</li>
                        <li>Use this information to identify resource-hungry services</li>
                    </ol>

                    <p><strong>Troubleshooting Common Issues:</strong></p>
                    <ul>
                        <li><strong>Container Won't Start:</strong> Check logs for error messages</li>
                        <li><strong>High Resource Usage:</strong> Identify which container is using excessive resources</li>
                        <li><strong>Network Issues:</strong> Verify port mappings and network configurations</li>
                        <li><strong>Storage Problems:</strong> Check volume mounts and storage usage</li>
                    </ul>

                    <p><strong>Creating Backups:</strong></p>
                    <ol>
                        <li>In Portainer, go to the "Volumes" section</li>
                        <li>Identify volumes containing important data</li>
                        <li>Use the "Browse" feature to explore volume contents</li>
                        <li>Download important files or use Docker commands to create backups</li>
                    </ol>
                </div>

                <!-- Advanced Multi-Service Integration -->
                <div class="integration-content" id="advanced-multiservice">
                    <h4>Building Advanced Multi-Service AI Workflows</h4>
                    <p>Combine multiple services from your AI Stack to create powerful, end-to-end AI workflows:</p>

                    <div class="important-note">
                        <h5>🤖 Unleashing the Full Potential</h5>
                        <p>By combining multiple services, you can:</p>
                        <ul>
                            <li>Create complex AI workflows that leverage the strengths of each tool</li>
                            <li>Build fully automated systems that process data from collection to insight</li>
                            <li>Develop custom AI solutions tailored to specific use cases</li>
                            <li>Maintain full control and privacy over your entire AI pipeline</li>
                        </ul>
                    </div>

                    <p><strong>Example: Automated Research Assistant</strong></p>
                    <p>This workflow combines multiple services to create an automated research assistant:</p>
                    <ol>
                        <li><strong>n8n:</strong> Orchestrates the entire workflow and handles scheduling</li>
                        <li><strong>SearXNG:</strong> Performs private web searches on your research topic</li>
                        <li><strong>Playwright:</strong> Scrapes detailed content from search results</li>
                        <li><strong>Ollama:</strong> Processes and analyzes the scraped content</li>
                        <li><strong>Flowise:</strong> Creates a chat interface to interact with the research results</li>
                        <li><strong>Supabase:</strong> Stores the processed research data for future reference</li>
                    </ol>

                    <p><strong>Implementation Steps:</strong></p>
                    <ol>
                        <li>Set up an n8n workflow that triggers on a schedule or manual input</li>
                        <li>Configure n8n to query SearXNG with your research topics</li>
                        <li>Use Playwright to scrape detailed content from the top search results</li>
                        <li>Send the scraped content to Ollama for analysis and summarization</li>
                        <li>Store the processed data in Supabase with proper categorization</li>
                        <li>Create a Flowise chatflow that can query the Supabase database</li>
                        <li>Add a chat interface that allows natural language queries about your research</li>
                    </ol>

                    <p><strong>Example: Content Creation Pipeline</strong></p>
                    <p>This workflow helps you create content based on YouTube videos:</p>
                    <ol>
                        <li><strong>YouTube Downloader:</strong> Downloads videos on topics of interest</li>
                        <li><strong>Ollama (Whisper):</strong> Transcribes the video content</li>
                        <li><strong>Ollama (LLM):</strong> Analyzes the transcript and generates new content</li>
                        <li><strong>n8n:</strong> Formats and publishes the content to various platforms</li>
                    </ol>

                    <p><strong>Advanced Integration Tips:</strong></p>
                    <ul>
                        <li><strong>Use Docker Networks:</strong> Ensure all services can communicate efficiently</li>
                        <li><strong>Implement Error Handling:</strong> Add retry logic and error notifications</li>
                        <li><strong>Monitor Resource Usage:</strong> Use Portainer to ensure services have adequate resources</li>
                        <li><strong>Create Documentation:</strong> Document your workflows for future reference</li>
                        <li><strong>Implement Security:</strong> Secure sensitive data and API endpoints</li>
                    </ul>
                </div>

                <!-- Supabase Setup Guide -->
                <div class="integration-content" id="supabase-setup">
                    <h4>Setting Up Your Supabase Database for AI Applications</h4>
                    <p>Learn how to configure and use Supabase as a powerful backend for your AI applications:</p>

                    <div class="important-note">
                        <h5>💾 Why Supabase for AI Applications</h5>
                        <p>Supabase provides essential infrastructure for AI apps:</p>
                        <ul>
                            <li>Store and retrieve AI-generated content</li>
                            <li>Manage user data and preferences</li>
                            <li>Create vector stores for semantic search</li>
                            <li>Build authentication systems for your AI tools</li>
                            <li>Set up real-time updates for collaborative AI applications</li>
                        </ul>
                    </div>

                    <p><strong>Getting Started with Supabase:</strong></p>
                    <ol>
                        <li>Access Supabase at <code class="copyable" data-copy="http://localhost:8000">http://localhost:8000</code></li>
                        <li>Create a new project and set up your admin credentials</li>
                        <li>Navigate to the SQL Editor to create your database schema</li>
                        <li>Use the Table Editor to manage your data visually</li>
                    </ol>

                    <p><strong>Creating a Table for AI Content:</strong></p>
                    <pre><code class="copyable" data-copy="CREATE TABLE ai_content (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);">CREATE TABLE ai_content (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);</code></pre>

                    <p><strong>Setting Up Vector Search (pgvector):</strong></p>
                    <pre><code class="copyable" data-copy="-- Enable the vector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create a table with vector column
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  content TEXT NOT NULL,
  embedding VECTOR(1536),
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);">-- Enable the vector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create a table with vector column
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  content TEXT NOT NULL,
  embedding VECTOR(1536),
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);</code></pre>

                    <p><strong>Creating an API for Your AI App:</strong></p>
                    <ol>
                        <li>Go to the API section in Supabase</li>
                        <li>Generate API keys for your application</li>
                        <li>Use the auto-generated API documentation to understand endpoints</li>
                        <li>Test API calls directly from the Supabase interface</li>
                    </ol>

                    <p><strong>Example: Storing AI-Generated Content:</strong></p>
                    <pre><code class="copyable" data-copy="// JavaScript example using Supabase JS client
const { data, error } = await supabase
  .from('ai_content')
  .insert([
    {
      title: 'AI-Generated Summary',
      content: 'This is a summary created by an AI model...',
      metadata: {
        model: 'llama3',
        prompt: 'Summarize the following text...',
        tokens: 1250
      }
    }
  ]);">// JavaScript example using Supabase JS client
const { data, error } = await supabase
  .from('ai_content')
  .insert([
    {
      title: 'AI-Generated Summary',
      content: 'This is a summary created by an AI model...',
      metadata: {
        model: 'llama3',
        prompt: 'Summarize the following text...',
        tokens: 1250
      }
    }
  ]);</code></pre>

                    <p><strong>Security Best Practices:</strong></p>
                    <ul>
                        <li>Use Row-Level Security (RLS) to control access to your data</li>
                        <li>Create specific API keys with limited permissions</li>
                        <li>Implement proper authentication for user-specific data</li>
                        <li>Regularly backup your database using Supabase's tools</li>
                    </ul>
                </div>

                <!-- Building RAG Systems -->
                <div class="integration-content" id="advanced-rag">
                    <h4>Building Powerful RAG Systems with Your AI Stack</h4>
                    <p>Create Retrieval-Augmented Generation (RAG) systems to enhance AI responses with your own knowledge base:</p>

                    <div class="important-note">
                        <h5>📖 Supercharging AI with Your Knowledge</h5>
                        <p>RAG systems combine the best of both worlds:</p>
                        <ul>
                            <li>Leverage the general knowledge of large language models</li>
                            <li>Augment responses with your specific documents and data</li>
                            <li>Get accurate, up-to-date answers based on your information</li>
                            <li>Reduce hallucinations by grounding AI in your trusted sources</li>
                        </ul>
                    </div>

                    <p><strong>RAG System Components:</strong></p>
                    <ol>
                        <li><strong>Document Loader:</strong> Import your documents (PDFs, web pages, etc.)</li>
                        <li><strong>Text Splitter:</strong> Chunk documents into manageable pieces</li>
                        <li><strong>Embedding Model:</strong> Convert text chunks into vector embeddings</li>
                        <li><strong>Vector Store:</strong> Store and index embeddings for retrieval</li>
                        <li><strong>Retriever:</strong> Find relevant documents based on queries</li>
                        <li><strong>Language Model:</strong> Generate responses using retrieved context</li>
                    </ol>

                    <p><strong>Building a RAG System with Flowise:</strong></p>
                    <ol>
                        <li>Create a new chatflow in Flowise</li>
                        <li>Add document loaders for your knowledge sources</li>
                        <li>Connect to a text splitter node (e.g., RecursiveCharacterTextSplitter)</li>
                        <li>Add an embedding node (e.g., Ollama embeddings)</li>
                        <li>Connect to a vector store node (e.g., Chroma)</li>
                        <li>Add a retrieval node to fetch relevant documents</li>
                        <li>Connect to an LLM node (e.g., Ollama with llama3)</li>
                        <li>Create a prompt template that includes retrieved context</li>
                        <li>Connect to a chat node for the user interface</li>
                    </ol>

                    <p><strong>Example RAG Prompt Template:</strong></p>
                    <pre><code class="copyable" data-copy="You are a helpful assistant with access to the following information. Use this information to answer the user's question. If you don't know the answer based on the provided information, say so clearly rather than making up an answer.

Context information:
{{context}}

User question: {{question}}

Your answer:">You are a helpful assistant with access to the following information. Use this information to answer the user's question. If you don't know the answer based on the provided information, say so clearly rather than making up an answer.

Context information:
{{context}}

User question: {{question}}

Your answer:</code></pre>

                    <p><strong>Advanced RAG Techniques:</strong></p>
                    <ul>
                        <li><strong>Hybrid Search:</strong> Combine keyword and semantic search for better retrieval</li>
                        <li><strong>Re-ranking:</strong> Use a second model to re-rank retrieved documents by relevance</li>
                        <li><strong>Query Expansion:</strong> Generate multiple versions of the query to improve recall</li>
                        <li><strong>Contextual Compression:</strong> Extract only the most relevant parts of retrieved documents</li>
                        <li><strong>Metadata Filtering:</strong> Filter documents based on metadata (date, source, etc.)</li>
                    </ul>

                    <p><strong>Performance Optimization:</strong></p>
                    <ul>
                        <li>Use smaller chunk sizes (e.g., 256-512 tokens) for more precise retrieval</li>
                        <li>Implement caching to speed up frequent queries</li>
                        <li>Choose efficient embedding models that balance quality and speed</li>
                        <li>Use vector stores with approximate nearest neighbor search for large datasets</li>
                    </ul>
                </div>

                <!-- Ollama Models Management -->
                <div class="integration-content" id="ollama-models">
                    <h4>Managing and Using Ollama Models</h4>
                    <p>Learn how to download, use, and manage different AI models with Ollama:</p>

                    <div class="important-note">
                        <h5>🤖 Getting Started with Local AI Models</h5>
                        <p>Ollama makes it easy to run powerful AI models on your own computer:</p>
                        <ul>
                            <li>No technical expertise required to get started</li>
                            <li>Complete privacy - your data never leaves your computer</li>
                            <li>No API costs or usage limits</li>
                            <li>Access to a wide variety of open-source models</li>
                        </ul>
                    </div>

                    <p><strong>Downloading Your First Model:</strong></p>
                    <ol>
                        <li>Open a terminal or command prompt</li>
                        <li>Run the command: <code class="copyable" data-copy="ollama pull llama3">ollama pull llama3</code></li>
                        <li>Wait for the download to complete (this may take a few minutes)</li>
                        <li>Once finished, the model is ready to use</li>
                    </ol>

                    <p><strong>Popular Models to Try:</strong></p>
                    <ul>
                        <li><strong>llama3</strong> - Meta's latest general-purpose model, good for most tasks</li>
                        <li><strong>mistral</strong> - Excellent performance with lower resource requirements</li>
                        <li><strong>phi3</strong> - Microsoft's efficient model, works well on lower-end hardware</li>
                        <li><strong>codellama</strong> - Specialized for programming and code generation</li>
                        <li><strong>whisper</strong> - For speech recognition and transcription</li>
                    </ul>

                    <p><strong>Chatting with a Model:</strong></p>
                    <pre><code class="copyable" data-copy="ollama run llama3">ollama run llama3</code></pre>
                    <p>Then type your questions or prompts and press Enter. Type <code>exit</code> to end the conversation.</p>

                    <p><strong>Listing Installed Models:</strong></p>
                    <pre><code class="copyable" data-copy="ollama list">ollama list</code></pre>

                    <p><strong>Removing a Model:</strong></p>
                    <pre><code class="copyable" data-copy="ollama rm modelname">ollama rm modelname</code></pre>

                    <p><strong>Using the Web Interface:</strong></p>
                    <ol>
                        <li>Access the Ollama web UI at <code class="copyable" data-copy="http://localhost:11434">http://localhost:11434</code></li>
                        <li>Select a model from the dropdown menu</li>
                        <li>Type your message in the input field and press Enter</li>
                        <li>The model will respond in a conversational format</li>
                    </ol>

                    <p><strong>Adjusting Model Parameters:</strong></p>
                    <ul>
                        <li><strong>Temperature</strong> (0.0 to 1.0): Lower values make responses more focused and deterministic, higher values make them more creative and varied</li>
                        <li><strong>Context Length</strong>: How much previous conversation the model remembers</li>
                        <li><strong>System Prompt</strong>: Instructions that guide the model's behavior</li>
                    </ul>

                    <p><strong>Troubleshooting Common Issues:</strong></p>
                    <ul>
                        <li><strong>Model is slow</strong>: Try a smaller model like phi3:mini or mistral:7b</li>
                        <li><strong>Out of memory errors</strong>: Use a model with fewer parameters or close other applications</li>
                        <li><strong>Model not found</strong>: Check your spelling or run <code>ollama pull modelname</code> to download it</li>
                        <li><strong>Connection issues</strong>: Make sure the Ollama service is running</li>
                    </ul>
                </div>

                <!-- Hugging Face Customization -->
                <div class="integration-content" id="huggingface-customization">
                    <h4>Customizing the Hugging Face Chat UI</h4>
                    <p>Learn how to personalize and get the most out of the Hugging Face Chat interface:</p>

                    <div class="important-note">
                        <h5>💬 Creating Your Ideal Chat Experience</h5>
                        <p>The Hugging Face Chat UI offers many customization options:</p>
                        <ul>
                            <li>Connect to different AI models and APIs</li>
                            <li>Customize the appearance and behavior</li>
                            <li>Create and save different conversation presets</li>
                            <li>Share conversations with others</li>
                        </ul>
                    </div>

                    <p><strong>Accessing the Chat UI:</strong></p>
                    <ol>
                        <li>Open your browser and go to <code class="copyable" data-copy="http://localhost:3000">http://localhost:3000</code></li>
                        <li>You'll see the main chat interface with a message input at the bottom</li>
                        <li>Click the model selector in the top-right corner to change models</li>
                    </ol>

                    <p><strong>Connecting to Different Models:</strong></p>
                    <ol>
                        <li>Click the model selector dropdown in the top-right corner</li>
                        <li>Choose from available options or select "Custom API"</li>
                        <li>For custom APIs, enter the endpoint URL and parameters</li>
                        <li>For Ollama, use <code class="copyable" data-copy="http://host.docker.internal:11434/api/generate">http://host.docker.internal:11434/api/generate</code></li>
                    </ol>

                    <p><strong>Creating a System Prompt:</strong></p>
                    <ol>
                        <li>Click the settings icon (gear) in the top-right corner</li>
                        <li>Select "System Prompt" from the menu</li>
                        <li>Enter instructions for how the AI should behave</li>
                        <li>Click "Save" to apply your system prompt</li>
                    </ol>

                    <p><strong>Example System Prompts:</strong></p>
                    <pre><code class="copyable" data-copy="You are a helpful, friendly assistant who provides accurate and concise information. If you're unsure about something, admit it rather than making up an answer.">You are a helpful, friendly assistant who provides accurate and concise information. If you're unsure about something, admit it rather than making up an answer.</code></pre>

                    <pre><code class="copyable" data-copy="You are a coding expert who helps explain programming concepts and debug code. Provide examples when helpful and explain your reasoning step by step.">You are a coding expert who helps explain programming concepts and debug code. Provide examples when helpful and explain your reasoning step by step.</code></pre>

                    <p><strong>Adjusting Model Parameters:</strong></p>
                    <ol>
                        <li>Click the settings icon in the top-right corner</li>
                        <li>Select "Parameters" from the menu</li>
                        <li>Adjust settings like temperature, top_p, max tokens, etc.</li>
                        <li>Click "Save" to apply your changes</li>
                    </ol>

                    <p><strong>Parameter Recommendations:</strong></p>
                    <ul>
                        <li><strong>For factual responses:</strong> Temperature: 0.1-0.3, Top P: 0.9</li>
                        <li><strong>For creative content:</strong> Temperature: 0.7-0.9, Top P: 0.9</li>
                        <li><strong>For balanced responses:</strong> Temperature: 0.5, Top P: 0.9</li>
                    </ul>

                    <p><strong>Saving and Sharing Conversations:</strong></p>
                    <ol>
                        <li>Click the "Share" button at the top of the chat</li>
                        <li>Choose whether to include system messages</li>
                        <li>Copy the generated link to share with others</li>
                        <li>Or use the "Download" option to save the conversation locally</li>
                    </ol>
                </div>

                <!-- SearXNG Customization -->
                <div class="integration-content" id="searxng-customization">
                    <h4>Customizing Your SearXNG Search Engine</h4>
                    <p>Learn how to configure and personalize your private search engine:</p>

                    <div class="important-note">
                        <h5>🔍 Private Search, Your Way</h5>
                        <p>SearXNG offers extensive customization options:</p>
                        <ul>
                            <li>Choose which search engines to use</li>
                            <li>Customize the appearance and behavior</li>
                            <li>Set up filters and safe search options</li>
                            <li>Create search shortcuts for faster access</li>
                        </ul>
                    </div>

                    <p><strong>Accessing SearXNG:</strong></p>
                    <ol>
                        <li>Open your browser and go to <code class="copyable" data-copy="http://localhost:8080">http://localhost:8080</code></li>
                        <li>You'll see the main search interface with a search bar</li>
                        <li>Click the settings icon (gear) in the top-right corner to access preferences</li>
                    </ol>

                    <p><strong>Choosing Search Engines:</strong></p>
                    <ol>
                        <li>Go to Preferences > Search engines</li>
                        <li>Check or uncheck boxes to enable or disable specific search engines</li>
                        <li>You can select engines by category (General, Images, News, etc.)</li>
                        <li>Click "Save" to apply your changes</li>
                    </ol>

                    <p><strong>Customizing the Interface:</strong></p>
                    <ol>
                        <li>Go to Preferences > User interface</li>
                        <li>Choose your preferred theme (Light, Dark, or Auto)</li>
                        <li>Select result display options (Simple, Full, or Custom)</li>
                        <li>Adjust the number of results per page</li>
                    </ol>

                    <p><strong>Setting Up Search Shortcuts:</strong></p>
                    <p>SearXNG supports special syntax for targeted searches:</p>
                    <ul>
                        <li><code class="copyable" data-copy="!g keyword">!g keyword</code> - Search only on Google</li>
                        <li><code class="copyable" data-copy="!b keyword">!b keyword</code> - Search only on Bing</li>
                        <li><code class="copyable" data-copy="!yt keyword">!yt keyword</code> - Search only on YouTube</li>
                        <li><code class="copyable" data-copy="!w keyword">!w keyword</code> - Search only on Wikipedia</li>
                    </ul>

                    <p><strong>Advanced Search Operators:</strong></p>
                    <ul>
                        <li><code class="copyable" data-copy="site:example.com keyword">site:example.com keyword</code> - Search only within a specific website</li>
                        <li><code class="copyable" data-copy="filetype:pdf keyword">filetype:pdf keyword</code> - Search for specific file types</li>
                        <li><code class="copyable" data-copy="intitle:keyword">intitle:keyword</code> - Search for pages with keyword in the title</li>
                        <li><code class="copyable" data-copy=""keyword1" "keyword2"">"keyword1" "keyword2"</code> - Search for exact phrases</li>
                    </ul>

                    <p><strong>Privacy Settings:</strong></p>
                    <ol>
                        <li>Go to Preferences > Privacy</li>
                        <li>Choose whether to save your search history</li>
                        <li>Configure URL handling (remove tracking parameters)</li>
                        <li>Set up autocomplete options</li>
                    </ol>

                    <p><strong>Creating Custom Filters:</strong></p>
                    <ol>
                        <li>Go to Preferences > Engines</li>
                        <li>Scroll down to "Engine tokens"</li>
                        <li>Create custom filters to include or exclude specific engines</li>
                        <li>Use these filters in your searches with the <code>&amp;engines=filter_name</code> parameter</li>
                    </ol>
                </div>

                <!-- YouTube Batch Processing -->
                <div class="integration-content" id="youtube-batch">
                    <h4>Batch Processing YouTube Videos</h4>
                    <p>Learn how to download and process multiple YouTube videos efficiently:</p>

                    <div class="important-note">
                        <h5>🎥 Building Your Video Library</h5>
                        <p>Batch processing allows you to:</p>
                        <ul>
                            <li>Download entire playlists or channels</li>
                            <li>Process multiple videos with consistent settings</li>
                            <li>Extract audio from multiple videos at once</li>
                            <li>Save time with automated workflows</li>
                        </ul>
                    </div>

                    <p><strong>Downloading a Playlist:</strong></p>
                    <ol>
                        <li>Open the YouTube Downloader at <code class="copyable" data-copy="http://localhost:8998">http://localhost:8998</code></li>
                        <li>Find a YouTube playlist URL (e.g., <code>https://www.youtube.com/playlist?list=PLAYLIST_ID</code>)</li>
                        <li>Paste the playlist URL in the input field</li>
                        <li>Select your preferred format and quality</li>
                        <li>Click "Download" to start the batch process</li>
                    </ol>

                    <p><strong>Using Command Line for Batch Downloads:</strong></p>
                    <p>For more advanced batch processing, you can use the command line:</p>
                    <pre><code class="copyable" data-copy="yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' --yes-playlist https://www.youtube.com/playlist?list=PLAYLIST_ID">yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' --yes-playlist https://www.youtube.com/playlist?list=PLAYLIST_ID</code></pre>

                    <p><strong>Extracting Audio from Multiple Videos:</strong></p>
                    <pre><code class="copyable" data-copy="yt-dlp -x --audio-format mp3 --yes-playlist https://www.youtube.com/playlist?list=PLAYLIST_ID">yt-dlp -x --audio-format mp3 --yes-playlist https://www.youtube.com/playlist?list=PLAYLIST_ID</code></pre>

                    <p><strong>Downloading Videos from a Channel:</strong></p>
                    <pre><code class="copyable" data-copy="yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' https://www.youtube.com/c/CHANNEL_NAME/videos">yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' https://www.youtube.com/c/CHANNEL_NAME/videos</code></pre>

                    <p><strong>Limiting the Number of Videos:</strong></p>
                    <pre><code class="copyable" data-copy="yt-dlp --playlist-end 5 https://www.youtube.com/playlist?list=PLAYLIST_ID">yt-dlp --playlist-end 5 https://www.youtube.com/playlist?list=PLAYLIST_ID</code></pre>

                    <p><strong>Creating a Batch File for Windows:</strong></p>
                    <ol>
                        <li>Create a new text file with a .bat extension (e.g., <code>download_videos.bat</code>)</li>
                        <li>Add your yt-dlp commands to the file, one per line</li>
                        <li>Save the file and double-click to run the batch process</li>
                    </ol>

                    <p><strong>Example Batch File:</strong></p>
                    <pre><code class="copyable" data-copy="@echo off
echo Starting batch download...

yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' https://www.youtube.com/playlist?list=PLAYLIST_ID_1

yt-dlp -x --audio-format mp3 https://www.youtube.com/playlist?list=PLAYLIST_ID_2

echo Batch download complete!
pause">@echo off
echo Starting batch download...

yt-dlp -f 'bestvideo[height<=720]+bestaudio/best[height<=720]' https://www.youtube.com/playlist?list=PLAYLIST_ID_1

yt-dlp -x --audio-format mp3 https://www.youtube.com/playlist?list=PLAYLIST_ID_2

echo Batch download complete!
pause</code></pre>

                    <p><strong>Processing Downloaded Videos:</strong></p>
                    <ul>
                        <li>Use ffmpeg to convert formats or extract audio</li>
                        <li>Create thumbnails or preview images</li>
                        <li>Split long videos into smaller segments</li>
                        <li>Add custom metadata to your video files</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Text Reading Bot -->
                <div class="integration-content" id="aibot-text">
                    <h4>Create Your Own Smart Reading Helper (Text Bot)</h4>
                    <p>Let's make an AI friend that can read text for you and tell you what it means in simple words!</p>

                    <div class="important-note">
                        <h5>📜 What Can Your Reading Helper Do?</h5>
                        <p>Your text reading helper can:</p>
                        <ul>
                            <li>Make long articles short and easy to understand</li>
                            <li>Find the important parts so you don't have to read everything</li>
                            <li>Answer your questions about what you're reading</li>
                            <li>Tell you if something sounds happy, sad, or angry</li>
                            <li>Find the main ideas and topics</li>
                        </ul>
                    </div>

                    <p><strong>Best AI Brains for Your Reading Helper:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>AI Brain</th>
                            <th>Where to Get It</th>
                            <th>What It's Good At</th>
                            <th>How to Install It</th>
                        </tr>
                        <tr>
                            <td>Llama 3 (70B)</td>
                            <td>Ollama</td>
                            <td>Really good at understanding hard text</td>
                            <td><code class="copyable" data-copy="ollama pull llama3:70b">ollama pull llama3:70b</code></td>
                        </tr>
                        <tr>
                            <td>Mistral (7B)</td>
                            <td>Ollama</td>
                            <td>Fast and still pretty smart</td>
                            <td><code class="copyable" data-copy="ollama pull mistral">ollama pull mistral</code></td>
                        </tr>
                        <tr>
                            <td>Phi-3 (14B)</td>
                            <td>Ollama</td>
                            <td>Good thinking skills, works on slower computers</td>
                            <td><code class="copyable" data-copy="ollama pull phi3:14b">ollama pull phi3:14b</code></td>
                        </tr>
                        <tr>
                            <td>Mixtral (8x7B)</td>
                            <td>Hugging Face</td>
                            <td>Super smart for detailed reading</td>
                            <td><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" target="_blank">Click here</a></td>
                        </tr>
                    </table>

                    <p><strong>Building Your Reading Helper (Step by Step):</strong></p>
                    <ol>
                        <li>Open Flowise by clicking <a href="http://localhost:3000" target="_blank">http://localhost:3000</a></li>
                        <li>Click the big "+" button to create a new chatflow</li>
                        <li>Give it a name like "My Reading Helper" and click "Create"</li>
                        <li>You'll see a blank canvas where we'll build our helper</li>
                        <li>Click the "+" button in the top right of the canvas</li>
                        <li>Type "text loader" in the search box and click on the "Text Loader" node</li>
                        <li>Click the "+" button at the bottom of the Text Loader node</li>
                        <li>Search for "text splitter" and add the "Text Splitter" node</li>
                        <li>Click the "+" button at the bottom of the Text Splitter node</li>
                        <li>Search for "embeddings" and add the "Ollama Embeddings" node</li>
                        <li>Click the "+" button at the bottom of the Embeddings node</li>
                        <li>Search for "vector store" and add the "Memory Vector Store" node</li>
                        <li>Click the "+" button at the bottom of the Vector Store node</li>
                        <li>Search for "retrieval" and add the "Retrieval QA Chain" node</li>
                        <li>Click the "+" button at the bottom of the Retrieval node</li>
                        <li>Search for "chat model" and add the "Ollama Chat Model" node</li>
                        <li>In the Ollama Chat Model settings, type the name of the model you installed (like "llama3" or "mistral")</li>
                        <li>Click the "+" button at the bottom of the Chat Model node</li>
                        <li>Search for "chat" and add the "Chat" node</li>
                        <li>Click "Save" at the top right of the screen</li>
                    </ol>

                    <p><strong>Teaching Your Reading Helper How to Help You:</strong></p>
                    <p>Click on the Retrieval QA Chain node and paste this in the "System Prompt" box (click to copy):</p>
                    <pre><code class="copyable" data-copy="You are a friendly reading helper. Your job is to help people understand text. Use the information I give you to answer questions in a clear, simple way. If you don't know something from the information I gave you, just say 'I don't see that in the text I was given.'\n\nHere's the text information: {{context}}\n\nQuestion: {{question}}\n\nGive a helpful, easy-to-understand answer.">You are a friendly reading helper. Your job is to help people understand text. Use the information I give you to answer questions in a clear, simple way. If you don't know something from the information I gave you, just say 'I don't see that in the text I was given.'\n\nHere's the text information: {{context}}\n\nQuestion: {{question}}\n\nGive a helpful, easy-to-understand answer.</code></pre>

                    <p><strong>Using Your Reading Helper:</strong></p>
                    <ol>
                        <li>Click the "Chat" button at the top right to open your reading helper</li>
                        <li>Copy and paste some text you want help with (like a news article or story)</li>
                        <li>Wait a moment while your helper reads the text</li>
                        <li>Now ask questions about what you pasted, like:</li>
                        <ul>
                            <li>"Can you make this shorter and easier to understand?"</li>
                            <li>"What are the most important parts?"</li>
                            <li>"What is this text mainly about?"</li>
                            <li>"Does this sound happy or sad?"</li>
                            <li>"What does this part mean?"</li>
                        </ul>
                    </ol>

                    <p><strong>Fun Things to Try with Your Reading Helper:</strong></p>
                    <ul>
                        <li>"Please explain this text like I'm 5 years old."</li>
                        <li>"What are the 3 most important things to remember from this?"</li>
                        <li>"Find all the numbers and facts in this text."</li>
                        <li>"What does the writer think about this topic?"</li>
                        <li>"Are there any parts that don't make sense or seem wrong?"</li>
                    </ul>

                    <p><strong>If Something Goes Wrong:</strong></p>
                    <ul>
                        <li><strong>If your helper seems confused:</strong> Try pasting a smaller amount of text</li>
                        <li><strong>If it's too slow:</strong> Try using a smaller model like Mistral instead of Llama</li>
                        <li><strong>If it doesn't answer your question:</strong> Try asking in a different way</li>
                        <li><strong>If it stops in the middle of answering:</strong> Try increasing the "Max Tokens" in the Ollama Chat Model settings</li>
                    </ul>
                </div>

                <!-- AI Bot Types - PDF Reading Bot -->
                <div class="integration-content" id="aibot-pdf">
                    <h4>Make Your Own PDF Book Reader Bot!</h4>
                    <p>Let's create a friendly AI helper that can read PDF files for you and tell you what's inside them!</p>

                    <div class="important-note">
                        <h5>📑 What Your PDF Helper Can Do</h5>
                        <p>Your PDF reading friend can:</p>
                        <ul>
                            <li>Read text from scanned books and papers</li>
                            <li>Help you find things in long PDF books</li>
                            <li>Make short summaries of big reports</li>
                            <li>Find pictures, tables, and important information</li>
                            <li>Answer questions about any part of a PDF</li>
                        </ul>
                    </div>

                    <p><strong>Best AI Brains for Your PDF Helper:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>AI Brain</th>
                            <th>Where to Get It</th>
                            <th>What It's Good At</th>
                            <th>How to Install It</th>
                        </tr>
                        <tr>
                            <td>Llama 3 (70B)</td>
                            <td>Ollama</td>
                            <td>Really good with school papers and hard books</td>
                            <td><code class="copyable" data-copy="ollama pull llama3:70b">ollama pull llama3:70b</code></td>
                        </tr>
                        <tr>
                            <td>Mixtral (8x7B)</td>
                            <td>Ollama</td>
                            <td>Good with technical books and finding details</td>
                            <td><code class="copyable" data-copy="ollama pull mixtral">ollama pull mixtral</code></td>
                        </tr>
                        <tr>
                            <td>Claude 3 Opus</td>
                            <td>Hugging Face</td>
                            <td>Really understands how books are organized</td>
                            <td><a href="https://huggingface.co/anthropic/claude-3-opus-20240229" target="_blank">Click here</a></td>
                        </tr>
                        <tr>
                            <td>GPT-4o</td>
                            <td>Hugging Face</td>
                            <td>Great at seeing pictures and complicated pages</td>
                            <td><a href="https://huggingface.co/openai/gpt-4o" target="_blank">Click here</a></td>
                        </tr>
                    </table>

                    <p><strong>Let's Build Your PDF Helper (Easy Steps):</strong></p>
                    <ol>
                        <li>Open Flowise by clicking <a href="http://localhost:3000" target="_blank">http://localhost:3000</a></li>
                        <li>Click the big "+" button to create a new chatflow</li>
                        <li>Name it something fun like "My PDF Reader" and click "Create"</li>
                        <li>You'll see a blank canvas where we'll build our helper</li>
                        <li>Click the "+" button in the top right of the canvas</li>
                        <li>Type "pdf loader" in the search box and click on the "PDF Loader" node</li>
                        <li>Click the "+" button at the bottom of the PDF Loader node</li>
                        <li>Search for "text splitter" and add the "Recursive Character Text Splitter" node</li>
                        <li>In the Text Splitter settings, set "Chunk Size" to 1000</li>
                        <li>Click the "+" button at the bottom of the Text Splitter node</li>
                        <li>Search for "embeddings" and add the "Ollama Embeddings" node</li>
                        <li>Click the "+" button at the bottom of the Embeddings node</li>
                        <li>Search for "vector store" and add the "Memory Vector Store" node</li>
                        <li>Click the "+" button at the bottom of the Vector Store node</li>
                        <li>Search for "retrieval" and add the "Conversational Retrieval QA" node</li>
                        <li>Click the "+" button at the bottom of the Retrieval node</li>
                        <li>Search for "chat model" and add the "Ollama Chat Model" node</li>
                        <li>In the Ollama Chat Model settings, type the name of the model you installed (like "llama3" or "mixtral")</li>
                        <li>Click the "+" button at the bottom of the Chat Model node</li>
                        <li>Search for "chat" and add the "Chat" node</li>
                        <li>Click "Save" at the top right of the screen</li>
                    </ol>

                    <p><strong>Teaching Your PDF Helper How to Be Helpful:</strong></p>
                    <p>Click on the Conversational Retrieval QA node and paste this in the "System Prompt" box (click to copy):</p>
                    <pre><code class="copyable" data-copy="You are a friendly PDF reading helper. Your job is to help people understand PDF documents. Use the information I give you to answer questions in a clear, simple way. If you see figures, tables, or special sections, mention them. If you don't know something from the information I gave you, just say 'I don't see that in the PDF I was given.'\n\nHere's the PDF information: {{context}}\n\nQuestion: {{question}}\n\nGive a helpful, easy-to-understand answer.">You are a friendly PDF reading helper. Your job is to help people understand PDF documents. Use the information I give you to answer questions in a clear, simple way. If you see figures, tables, or special sections, mention them. If you don't know something from the information I gave you, just say 'I don't see that in the PDF I was given.'\n\nHere's the PDF information: {{context}}\n\nQuestion: {{question}}\n\nGive a helpful, easy-to-understand answer.</code></pre>

                    <p><strong>Using Your PDF Helper:</strong></p>
                    <ol>
                        <li>Click the "Chat" button at the top right to open your PDF helper</li>
                        <li>Click the "Upload" button to upload a PDF file</li>
                        <li>Wait a moment while your helper reads the PDF</li>
                        <li>Now ask questions about the PDF, like:</li>
                        <ul>
                            <li>"What is this PDF about?"</li>
                            <li>"Can you summarize page 5?"</li>
                            <li>"What are the main points in this document?"</li>
                            <li>"Are there any pictures or tables?"</li>
                            <li>"What does it say about [specific topic]?"</li>
                        </ul>
                    </ol>

                    <p><strong>Different Kinds of PDFs Your Helper Can Read:</strong></p>
                    <ul>
                        <li><strong>School Books:</strong> Ask about chapters, main ideas, and important facts</li>
                        <li><strong>Stories and Novels:</strong> Ask about characters, plot, and what happens</li>
                        <li><strong>Instruction Manuals:</strong> Ask how to do things or what steps to follow</li>
                        <li><strong>Reports with Numbers:</strong> Ask about important numbers and what they mean</li>
                        <li><strong>Scanned Papers:</strong> It can even read most handwriting and old documents!</li>
                    </ul>

                    <p><strong>If Something Goes Wrong:</strong></p>
                    <ul>
                        <li><strong>If the PDF is too big:</strong> Try uploading just a few pages at a time</li>
                        <li><strong>If it can't read the PDF:</strong> Make sure the PDF isn't password protected</li>
                        <li><strong>If it gives wrong answers:</strong> Try asking your question in a different way</li>
                        <li><strong>If it's confused about pictures:</strong> Ask specifically about the text near the picture</li>
                        <li><strong>If it's slow:</strong> Try using a smaller model like Mistral instead of Llama</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Web Scraping Bot -->
                <div class="integration-content" id="aibot-webscraping">
                    <h4>Make Your Own Internet Explorer Bot!</h4>
                    <p>Let's create a friendly AI helper that can visit websites and collect information for you - like having your own research assistant!</p>

                    <div class="important-note">
                        <h5>🌐 What Your Internet Explorer Bot Can Do</h5>
                        <p>Your web explorer friend can:</p>
                        <ul>
                            <li>Visit lots of websites and collect information</li>
                            <li>Find specific things you're looking for on websites</li>
                            <li>Check websites for new information or changes</li>
                            <li>Gather information from many places for your research</li>
                            <li>Organize information it finds in a neat way</li>
                        </ul>
                    </div>

                    <p><strong>Tools We'll Use to Build Your Explorer Bot:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>Tool Name</th>
                            <th>What It Is</th>
                            <th>What It Does</th>
                            <th>Where to Find It</th>
                        </tr>
                        <tr>
                            <td>Playwright</td>
                            <td>Website Explorer</td>
                            <td>Visits websites and finds information</td>
                            <td><a href="http://localhost:3333" target="_blank">Click here</a></td>
                        </tr>
                        <tr>
                            <td>n8n</td>
                            <td>Workflow Builder</td>
                            <td>Connects all the tools together</td>
                            <td><a href="http://localhost:5678" target="_blank">Click here</a></td>
                        </tr>
                        <tr>
                            <td>Llama 3</td>
                            <td>AI Brain</td>
                            <td>Understands the information found</td>
                            <td><code class="copyable" data-copy="ollama pull llama3">ollama pull llama3</code></td>
                        </tr>
                        <tr>
                            <td>Supabase</td>
                            <td>Information Storage</td>
                            <td>Keeps all the information organized</td>
                            <td><a href="http://localhost:8000" target="_blank">Click here</a></td>
                        </tr>
                    </table>

                    <p><strong>Let's Build Your Internet Explorer Bot (Easy Steps):</strong></p>
                    <ol>
                        <li>Open n8n by clicking <a href="http://localhost:5678" target="_blank">http://localhost:5678</a></li>
                        <li>Click the big "+" button to create a new workflow</li>
                        <li>Name it something fun like "My Web Explorer" and click "Create"</li>
                        <li>You'll see a blank canvas where we'll build our explorer</li>
                        <li>Click the "+" button in the top right of the canvas</li>
                        <li>Type "http request" in the search box and click on the "HTTP Request" node</li>
                        <li>In the HTTP Request settings:</li>
                        <ul>
                            <li>For "Method" select "POST"</li>
                            <li>For "URL" type: <code class="copyable" data-copy="http://host.docker.internal:3333/api/scrape">http://host.docker.internal:3333/api/scrape</code></li>
                            <li>For "Request Format" select "JSON"</li>
                            <li>For "JSON/RAW Parameters" paste this simple example (click to copy):</li>
                        </ul>
                        <pre><code class="copyable" data-copy='{
  "url": "https://news.google.com",
  "selectors": [
    {
      "name": "headlines",
      "selector": "h3",
      "type": "text",
      "multiple": true
    }
  ]
}'>{"url":"https://news.google.com","selectors":[{"name":"headlines","selector":"h3","type":"text","multiple":true}]}</code></pre>
                        <li>Click "Done" to save this node</li>
                        <li>Click the "+" button at the bottom of the HTTP Request node</li>
                        <li>Search for "function" and add the "Function" node</li>
                        <li>In the Function node, paste this code (click to copy):</li>
                        <pre><code class="copyable" data-copy="// This makes the data look nice
const results = $input.item.body.results;

// Check if we got any headlines
if (results && results.headlines && results.headlines.length > 0) {
  // Make a list of all the headlines we found
  const headlines = results.headlines;

  // Return the headlines in a nice format
  return {
    foundHeadlines: headlines,
    count: headlines.length,
    source: $input.item.body.url,
    scrapedAt: new Date().toISOString()
  };
} else {
  // If we didn't find any headlines
  return {
    error: 'No headlines found',
    source: $input.item.body.url
  };
}">// This makes the data look nice
const results = $input.item.body.results;

// Check if we got any headlines
if (results && results.headlines && results.headlines.length > 0) {
  // Make a list of all the headlines we found
  const headlines = results.headlines;

  // Return the headlines in a nice format
  return {
    foundHeadlines: headlines,
    count: headlines.length,
    source: $input.item.body.url,
    scrapedAt: new Date().toISOString()
  };
} else {
  // If we didn't find any headlines
  return {
    error: 'No headlines found',
    source: $input.item.body.url
  };
}</code></pre>
                        <li>Click "Done" to save this node</li>
                        <li>Click the "+" button at the bottom of the Function node</li>
                        <li>Search for "http request" and add another "HTTP Request" node</li>
                        <li>In this HTTP Request node:</li>
                        <ul>
                            <li>For "Method" select "POST"</li>
                            <li>For "URL" type: <code class="copyable" data-copy="http://host.docker.internal:11434/api/generate">http://host.docker.internal:11434/api/generate</code></li>
                            <li>For "Request Format" select "JSON"</li>
                            <li>For "JSON/RAW Parameters" paste this (click to copy):</li>
                        </ul>
                        <pre><code class="copyable" data-copy='{
  "model": "llama3",
  "prompt": "You are a friendly news helper. I found these headlines from the web. Can you tell me what the main topics are and what seems to be happening in the news today? Here are the headlines: {{$node[\"Function\"].json[\"foundHeadlines\"].join(\", \")}}\n\nPlease give me a simple summary that a child could understand."
}'>{"model":"llama3","prompt":"You are a friendly news helper. I found these headlines from the web. Can you tell me what the main topics are and what seems to be happening in the news today? Here are the headlines: {{$node[\"Function\"].json[\"foundHeadlines\"].join(\", \")}}\n\nPlease give me a simple summary that a child could understand."}</code></pre>
                        <li>Click "Done" to save this node</li>
                        <li>Click "Save" at the top right of the screen</li>
                        <li>Click the "Execute Workflow" button to test it</li>
                    </ol>

                    <p><strong>Fun Things Your Explorer Bot Can Find:</strong></p>
                    <ul>
                        <li><strong>News Headlines:</strong> Find out what's happening in the world</li>
                        <li><strong>Weather Information:</strong> Check the weather in different places</li>
                        <li><strong>Recipes:</strong> Find yummy recipes from cooking websites</li>
                        <li><strong>Sports Scores:</strong> Get the latest game results</li>
                        <li><strong>Movie Information:</strong> Find new movies and their ratings</li>
                    </ul>

                    <p><strong>Being a Good Internet Citizen:</strong></p>
                    <ul>
                        <li><strong>Be Polite:</strong> Don't visit websites too quickly or too often</li>
                        <li><strong>Follow the Rules:</strong> Some websites don't want bots to visit them</li>
                        <li><strong>Give Credit:</strong> Always remember where your information came from</li>
                        <li><strong>Be Careful:</strong> Don't collect private information about people</li>
                        <li><strong>Ask Permission:</strong> For school projects, ask your teacher if it's okay</li>
                    </ul>

                    <p><strong>If Something Goes Wrong:</strong></p>
                    <ul>
                        <li><strong>If the website doesn't load:</strong> Try a different website</li>
                        <li><strong>If no information is found:</strong> Try changing the "selector" in your code</li>
                        <li><strong>If it's too slow:</strong> Some websites are very big and take time to explore</li>
                        <li><strong>If you get an error:</strong> Make sure all your code is copied correctly</li>
                        <li><strong>If the AI doesn't understand:</strong> Try giving it fewer headlines to analyze</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Multi-Modal Bot -->
                <div class="integration-content" id="aibot-multimodal">
                    <h4>Building a Multi-Modal AI Bot</h4>
                    <p>Create an advanced AI bot that can process and generate text, images, audio, and more:</p>

                    <div class="important-note">
                        <h5>📸 Beyond Text: Multi-Modal AI</h5>
                        <p>A multi-modal bot can help you:</p>
                        <ul>
                            <li>Analyze images and extract information from them</li>
                            <li>Process audio and video content</li>
                            <li>Generate images based on text descriptions</li>
                            <li>Transcribe and summarize audio recordings</li>
                            <li>Create comprehensive analyses using multiple data types</li>
                        </ul>
                    </div>

                    <p><strong>Recommended Models:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>Model</th>
                            <th>Platform</th>
                            <th>Capabilities</th>
                            <th>Resources</th>
                        </tr>
                        <tr>
                            <td>LLaVA (7B/13B)</td>
                            <td>Ollama</td>
                            <td>Text + Vision model, can analyze images</td>
                            <td><code class="copyable" data-copy="ollama pull llava">ollama pull llava</code></td>
                        </tr>
                        <tr>
                            <td>Bakllava</td>
                            <td>Ollama</td>
                            <td>Advanced vision capabilities with strong reasoning</td>
                            <td><code class="copyable" data-copy="ollama pull bakllava">ollama pull bakllava</code></td>
                        </tr>
                        <tr>
                            <td>Whisper</td>
                            <td>Ollama</td>
                            <td>Speech recognition and transcription</td>
                            <td><code class="copyable" data-copy="ollama pull whisper">ollama pull whisper</code></td>
                        </tr>
                        <tr>
                            <td>Stable Diffusion</td>
                            <td>Hugging Face</td>
                            <td>Image generation from text descriptions</td>
                            <td><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0" target="_blank">Link</a></td>
                        </tr>
                    </table>

                    <p><strong>Building a Multi-Modal Bot with Flowise:</strong></p>
                    <ol>
                        <li>Create a new chatflow in Flowise</li>
                        <li>Add input nodes for different data types (text, image, audio)</li>
                        <li>Connect to appropriate processing nodes for each data type</li>
                        <li>Add specialized models for each modality (e.g., LLaVA for images)</li>
                        <li>Create a unified output that combines insights from all modalities</li>
                        <li>Connect to a chat interface that supports multiple input types</li>
                    </ol>

                    <p><strong>Example Multi-Modal Workflow:</strong></p>
                    <ol>
                        <li><strong>Image Analysis:</strong> Upload an image to LLaVA for visual analysis</li>
                        <li><strong>Text Processing:</strong> Provide context or questions about the image</li>
                        <li><strong>Combined Analysis:</strong> Generate insights that incorporate both visual and textual information</li>
                        <li><strong>Optional Generation:</strong> Create new images based on the analysis</li>
                    </ol>

                    <p><strong>Using LLaVA for Image Analysis:</strong></p>
                    <pre><code class="copyable" data-copy="curl -X POST http://localhost:11434/api/generate -d '{"model":"llava","prompt":"What's in this image?","images":["<BASE64_ENCODED_IMAGE>"]}'">curl -X POST http://localhost:11434/api/generate -d '{"model":"llava","prompt":"What's in this image?","images":["<BASE64_ENCODED_IMAGE>"]}' </code></pre>

                    <p><strong>Using Whisper for Audio Transcription:</strong></p>
                    <pre><code class="copyable" data-copy="curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper">curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper</code></pre>

                    <p><strong>Multi-Modal Bot Use Cases:</strong></p>
                    <ul>
                        <li><strong>Content Creation:</strong> Generate images and text for marketing materials</li>
                        <li><strong>Research Analysis:</strong> Process research papers with text, charts, and images</li>
                        <li><strong>Media Monitoring:</strong> Analyze news articles, images, and videos</li>
                        <li><strong>Educational Tools:</strong> Create interactive learning experiences</li>
                        <li><strong>Accessibility:</strong> Convert between different modalities (text to speech, image to text)</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Video Analysis Bot -->
                <div class="integration-content" id="aibot-video">
                    <h4>Building a Video Analysis AI Bot</h4>
                    <p>Create an AI bot that can process, analyze, and extract insights from video content:</p>

                    <div class="important-note">
                        <h5>🎥 Video Intelligence</h5>
                        <p>A video analysis bot can help you:</p>
                        <ul>
                            <li>Transcribe speech from videos automatically</li>
                            <li>Extract key moments and highlights</li>
                            <li>Identify objects, people, and actions in videos</li>
                            <li>Generate summaries of video content</li>
                            <li>Create searchable indexes of video libraries</li>
                        </ul>
                    </div>

                    <p><strong>Recommended Models:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>Model</th>
                            <th>Platform</th>
                            <th>Capabilities</th>
                            <th>Resources</th>
                        </tr>
                        <tr>
                            <td>Whisper</td>
                            <td>Ollama</td>
                            <td>Speech recognition and transcription</td>
                            <td><code class="copyable" data-copy="ollama pull whisper">ollama pull whisper</code></td>
                        </tr>
                        <tr>
                            <td>LLaVA</td>
                            <td>Ollama</td>
                            <td>Visual understanding of video frames</td>
                            <td><code class="copyable" data-copy="ollama pull llava">ollama pull llava</code></td>
                        </tr>
                        <tr>
                            <td>Llama 3 (70B)</td>
                            <td>Ollama</td>
                            <td>Analysis of transcripts and content</td>
                            <td><code class="copyable" data-copy="ollama pull llama3:70b">ollama pull llama3:70b</code></td>
                        </tr>
                        <tr>
                            <td>VideoLLaMA</td>
                            <td>Hugging Face</td>
                            <td>End-to-end video understanding</td>
                            <td><a href="https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned" target="_blank">Link</a></td>
                        </tr>
                    </table>

                    <p><strong>Building Your Video Analysis Bot:</strong></p>
                    <ol>
                        <li>Download videos using the YouTube Downloader</li>
                        <li>Extract audio from the video using ffmpeg</li>
                        <li>Transcribe the audio using Whisper</li>
                        <li>Extract frames from the video at regular intervals</li>
                        <li>Analyze frames using LLaVA or another vision model</li>
                        <li>Combine transcript and visual analysis using an LLM</li>
                        <li>Generate insights, summaries, or answers to specific questions</li>
                    </ol>

                    <p><strong>Extracting Audio from Video:</strong></p>
                    <pre><code class="copyable" data-copy="ffmpeg -i video.mp4 -q:a 0 -map a audio.mp3">ffmpeg -i video.mp4 -q:a 0 -map a audio.mp3</code></pre>

                    <p><strong>Transcribing with Whisper:</strong></p>
                    <pre><code class="copyable" data-copy="curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper">curl -X POST http://localhost:11434/api/audio -H 'Content-Type: multipart/form-data' -F file=@audio.mp3 -F model=whisper</code></pre>

                    <p><strong>Extracting Frames from Video:</strong></p>
                    <pre><code class="copyable" data-copy="ffmpeg -i video.mp4 -vf "fps=1/10" frames/frame_%04d.jpg">ffmpeg -i video.mp4 -vf "fps=1/10" frames/frame_%04d.jpg</code></pre>
                    <p>This extracts one frame every 10 seconds.</p>

                    <p><strong>Analyzing Video Content with n8n:</strong></p>
                    <ol>
                        <li>Create a new workflow in n8n</li>
                        <li>Add a "Read Binary File" node to load the video</li>
                        <li>Add an "Execute Command" node to extract audio and frames</li>
                        <li>Add HTTP Request nodes to call Whisper and LLaVA</li>
                        <li>Add a Function node to combine the results</li>
                        <li>Add another HTTP Request node to send to an LLM for final analysis</li>
                    </ol>

                    <p><strong>Example Prompt for Video Analysis:</strong></p>
                    <pre><code class="copyable" data-copy="You are an expert at analyzing video content. I've provided a transcript of the audio and descriptions of key frames from the video. Please analyze this content and provide the following:\n\n1. A comprehensive summary of the video\n2. Key topics and themes discussed\n3. Important visual elements and their significance\n4. A timeline of main events or points in the video\n5. Any notable quotes or statements\n\nTranscript:\n{{transcript}}\n\nKey Frames:\n{{frame_descriptions}}">You are an expert at analyzing video content. I've provided a transcript of the audio and descriptions of key frames from the video. Please analyze this content and provide the following:\n\n1. A comprehensive summary of the video\n2. Key topics and themes discussed\n3. Important visual elements and their significance\n4. A timeline of main events or points in the video\n5. Any notable quotes or statements\n\nTranscript:\n{{transcript}}\n\nKey Frames:\n{{frame_descriptions}}</code></pre>

                    <p><strong>Advanced Video Analysis Techniques:</strong></p>
                    <ul>
                        <li><strong>Scene Detection:</strong> Identify scene changes for better segmentation</li>
                        <li><strong>Action Recognition:</strong> Identify specific activities in the video</li>
                        <li><strong>Speaker Diarization:</strong> Identify who is speaking when</li>
                        <li><strong>Emotion Detection:</strong> Analyze facial expressions and tone of voice</li>
                        <li><strong>Content Moderation:</strong> Identify inappropriate content</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Computer Vision Bot -->
                <div class="integration-content" id="aibot-vision">
                    <h4>Building a Computer Vision AI Bot</h4>
                    <p>Create an AI bot that can see, understand, and analyze visual information:</p>

                    <div class="important-note">
                        <h5>📷 Visual Intelligence</h5>
                        <p>A computer vision bot can help you:</p>
                        <ul>
                            <li>Identify objects, people, and scenes in images</li>
                            <li>Read and extract text from images (OCR)</li>
                            <li>Detect faces and analyze expressions</li>
                            <li>Measure and analyze visual properties</li>
                            <li>Generate detailed descriptions of images</li>
                        </ul>
                    </div>

                    <p><strong>Recommended Models:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>Model</th>
                            <th>Platform</th>
                            <th>Capabilities</th>
                            <th>Resources</th>
                        </tr>
                        <tr>
                            <td>LLaVA (7B/13B)</td>
                            <td>Ollama</td>
                            <td>General visual understanding and description</td>
                            <td><code class="copyable" data-copy="ollama pull llava">ollama pull llava</code></td>
                        </tr>
                        <tr>
                            <td>Bakllava</td>
                            <td>Ollama</td>
                            <td>Advanced visual reasoning and analysis</td>
                            <td><code class="copyable" data-copy="ollama pull bakllava">ollama pull bakllava</code></td>
                        </tr>
                        <tr>
                            <td>CogVLM</td>
                            <td>Ollama</td>
                            <td>Detailed visual comprehension and reasoning</td>
                            <td><code class="copyable" data-copy="ollama pull cogvlm">ollama pull cogvlm</code></td>
                        </tr>
                        <tr>
                            <td>CLIP</td>
                            <td>Hugging Face</td>
                            <td>Image classification and retrieval</td>
                            <td><a href="https://huggingface.co/openai/clip-vit-base-patch32" target="_blank">Link</a></td>
                        </tr>
                    </table>

                    <p><strong>Using LLaVA for Image Analysis:</strong></p>
                    <pre><code class="copyable" data-copy="curl -X POST http://localhost:11434/api/generate -d '{"model":"llava","prompt":"Describe this image in detail","images":["<BASE64_ENCODED_IMAGE>"]}'">curl -X POST http://localhost:11434/api/generate -d '{"model":"llava","prompt":"Describe this image in detail","images":["<BASE64_ENCODED_IMAGE>"]}' </code></pre>

                    <p><strong>Building a Vision Bot with Flowise:</strong></p>
                    <ol>
                        <li>Create a new chatflow in Flowise</li>
                        <li>Add an "Image Input" node for handling image uploads</li>
                        <li>Connect to a "Vision Model" node (configured for LLaVA or another vision model)</li>
                        <li>Add a prompt template that instructs the model on how to analyze the image</li>
                        <li>Connect to a chat interface that supports image uploads</li>
                    </ol>

                    <p><strong>Example Vision Tasks and Prompts:</strong></p>
                    <ul>
                        <li><strong>General Description:</strong> <code class="copyable" data-copy="Describe this image in detail.">Describe this image in detail.</code></li>
                        <li><strong>Object Detection:</strong> <code class="copyable" data-copy="List all objects visible in this image.">List all objects visible in this image.</code></li>
                        <li><strong>Text Extraction:</strong> <code class="copyable" data-copy="Read and extract all text visible in this image.">Read and extract all text visible in this image.</code></li>
                        <li><strong>Scene Analysis:</strong> <code class="copyable" data-copy="Where was this photo taken? Describe the environment.">Where was this photo taken? Describe the environment.</code></li>
                        <li><strong>Technical Analysis:</strong> <code class="copyable" data-copy="Analyze the composition, lighting, and technical aspects of this photograph.">Analyze the composition, lighting, and technical aspects of this photograph.</code></li>
                    </ul>

                    <p><strong>Specialized Vision Applications:</strong></p>
                    <ul>
                        <li><strong>Document Analysis:</strong> Extract information from IDs, receipts, forms</li>
                        <li><strong>Medical Imaging:</strong> Analyze X-rays, MRIs, or other medical images</li>
                        <li><strong>Product Recognition:</strong> Identify products in images</li>
                        <li><strong>Quality Control:</strong> Detect defects or issues in manufacturing</li>
                        <li><strong>Security Monitoring:</strong> Identify unusual activities or objects</li>
                    </ul>

                    <p><strong>Building an Image Search System:</strong></p>
                    <ol>
                        <li>Process a collection of images with a vision model</li>
                        <li>Generate embeddings for each image</li>
                        <li>Store the embeddings in a vector database</li>
                        <li>Allow users to search by text description or similar image</li>
                        <li>Retrieve the most similar images based on embedding similarity</li>
                    </ol>

                    <p><strong>Advanced Vision Techniques:</strong></p>
                    <ul>
                        <li><strong>Image Segmentation:</strong> Identify precise boundaries of objects</li>
                        <li><strong>Depth Estimation:</strong> Understand the 3D structure of a scene</li>
                        <li><strong>Visual Question Answering:</strong> Answer specific questions about images</li>
                        <li><strong>Image-to-Image Translation:</strong> Convert images between different styles</li>
                        <li><strong>Few-Shot Learning:</strong> Recognize new objects with minimal examples</li>
                    </ul>
                </div>

                <!-- AI Bot Types - Fine-Tuning Models -->
                <div class="integration-content" id="aibot-finetuning">
                    <h4>Fine-Tuning AI Models for Your Specific Needs</h4>
                    <p>Learn how to customize pre-trained models to improve performance on your specific tasks:</p>

                    <div class="important-note">
                        <h5>🎛️ Customizing AI Models</h5>
                        <p>Fine-tuning allows you to:</p>
                        <ul>
                            <li>Adapt models to your specific domain or industry</li>
                            <li>Improve performance on specialized tasks</li>
                            <li>Teach models your preferred style or format</li>
                            <li>Reduce hallucinations on your specific data</li>
                            <li>Create more efficient, targeted AI solutions</li>
                        </ul>
                    </div>

                    <p><strong>What is Fine-Tuning?</strong></p>
                    <p>Fine-tuning is the process of taking a pre-trained model and further training it on a smaller, specialized dataset to adapt it to a specific task or domain. This allows you to leverage the general knowledge of large models while customizing them for your particular needs.</p>

                    <p><strong>When to Fine-Tune:</strong></p>
                    <ul>
                        <li><strong>Domain-Specific Knowledge:</strong> When you need the model to understand specialized terminology or concepts</li>
                        <li><strong>Consistent Format:</strong> When you want responses in a specific style or format</li>
                        <li><strong>Improved Accuracy:</strong> When you need better performance on specific tasks</li>
                        <li><strong>Reduced Hallucinations:</strong> When you need more factual responses in your domain</li>
                        <li><strong>Smaller Models:</strong> When you want to use smaller, more efficient models without sacrificing quality</li>
                    </ul>

                    <p><strong>Fine-Tuning with Ollama:</strong></p>
                    <p>Ollama allows you to create custom models using a Modelfile:</p>

                    <pre><code class="copyable" data-copy="FROM llama3

SYSTEM """You are a helpful AI assistant specialized in cybersecurity. You provide accurate, technical information about security concepts, vulnerabilities, and best practices. When asked about security issues, you prioritize explaining proper security measures rather than how to exploit vulnerabilities."""

# Add your custom training data
TRAIN """
User: What is a buffer overflow vulnerability?
Assistant: A buffer overflow vulnerability occurs when a program writes more data to a buffer (a temporary storage area) than it can hold. This can allow attackers to overwrite adjacent memory, potentially executing malicious code or crashing the system. Buffer overflows are common in languages like C and C++ that don't perform automatic bounds checking. Prevention methods include input validation, using safer programming languages with automatic memory management, employing address space layout randomization (ASLR), and implementing stack canaries to detect buffer overflows.

User: How can I secure my home Wi-Fi network?
Assistant: To secure your home Wi-Fi network:
1. Use WPA3 encryption if available, or at minimum WPA2
2. Create a strong, unique password (12+ characters with mixed types)
3. Change the default SSID (network name) and admin credentials
4. Enable the router's firewall and keep firmware updated
5. Consider setting up a guest network for visitors
6. Use MAC address filtering for an extra layer of security
7. Disable WPS (Wi-Fi Protected Setup) as it can be vulnerable
8. Position your router centrally to minimize signal outside your home

These measures will significantly improve your home network security against common threats.
"""

# Save the model with a custom name
TAG custom-cybersecurity-assistant">FROM llama3

SYSTEM """You are a helpful AI assistant specialized in cybersecurity. You provide accurate, technical information about security concepts, vulnerabilities, and best practices. When asked about security issues, you prioritize explaining proper security measures rather than how to exploit vulnerabilities."""

# Add your custom training data
TRAIN """
User: What is a buffer overflow vulnerability?
Assistant: A buffer overflow vulnerability occurs when a program writes more data to a buffer (a temporary storage area) than it can hold. This can allow attackers to overwrite adjacent memory, potentially executing malicious code or crashing the system. Buffer overflows are common in languages like C and C++ that don't perform automatic bounds checking. Prevention methods include input validation, using safer programming languages with automatic memory management, employing address space layout randomization (ASLR), and implementing stack canaries to detect buffer overflows.

User: How can I secure my home Wi-Fi network?
Assistant: To secure your home Wi-Fi network:
1. Use WPA3 encryption if available, or at minimum WPA2
2. Create a strong, unique password (12+ characters with mixed types)
3. Change the default SSID (network name) and admin credentials
4. Enable the router's firewall and keep firmware updated
5. Consider setting up a guest network for visitors
6. Use MAC address filtering for an extra layer of security
7. Disable WPS (Wi-Fi Protected Setup) as it can be vulnerable
8. Position your router centrally to minimize signal outside your home

These measures will significantly improve your home network security against common threats.
"""

# Save the model with a custom name
TAG custom-cybersecurity-assistant</code></pre>

                    <p><strong>Creating and Using a Custom Model:</strong></p>
                    <ol>
                        <li>Create a Modelfile like the example above</li>
                        <li>Save it to a file named <code>Modelfile</code></li>
                        <li>Run: <code class="copyable" data-copy="ollama create custom-model -f ./Modelfile">ollama create custom-model -f ./Modelfile</code></li>
                        <li>Use your model: <code class="copyable" data-copy="ollama run custom-model">ollama run custom-model</code></li>
                    </ol>

                    <p><strong>Preparing Training Data:</strong></p>
                    <p>Good training data is crucial for effective fine-tuning. Here are some tips:</p>
                    <ul>
                        <li><strong>Format:</strong> Use a consistent format (usually conversation pairs)</li>
                        <li><strong>Quality:</strong> Ensure high-quality, accurate information</li>
                        <li><strong>Diversity:</strong> Include a variety of examples covering your domain</li>
                        <li><strong>Quantity:</strong> Start with at least 10-50 examples (more is better)</li>
                        <li><strong>Balance:</strong> Cover different aspects of your domain evenly</li>
                    </ul>

                    <p><strong>Fine-Tuning Techniques:</strong></p>
                    <ul>
                        <li><strong>Instruction Fine-Tuning:</strong> Training on specific instructions and desired responses</li>
                        <li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> Using human preferences to guide training</li>
                        <li><strong>LoRA (Low-Rank Adaptation):</strong> Efficient fine-tuning that updates only a small number of parameters</li>
                        <li><strong>QLoRA:</strong> Quantized version of LoRA for even more efficient fine-tuning</li>
                        <li><strong>Prompt-Tuning:</strong> Learning soft prompts rather than modifying model weights</li>
                    </ul>

                    <p><strong>Advanced Fine-Tuning with External Tools:</strong></p>
                    <p>For more advanced fine-tuning, you can use specialized tools:</p>
                    <ul>
                        <li><strong>Axolotl:</strong> <a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank">GitHub Link</a> - Streamlined fine-tuning for various models</li>
                        <li><strong>LLaMA Factory:</strong> <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank">GitHub Link</a> - User-friendly fine-tuning for LLaMA models</li>
                        <li><strong>PEFT:</strong> <a href="https://github.com/huggingface/peft" target="_blank">GitHub Link</a> - Parameter-Efficient Fine-Tuning methods</li>
                    </ul>

                    <p><strong>Evaluating Your Fine-Tuned Model:</strong></p>
                    <ol>
                        <li>Create a test set separate from your training data</li>
                        <li>Define clear metrics for success (accuracy, relevance, etc.)</li>
                        <li>Compare performance against the base model</li>
                        <li>Get human feedback on the quality of responses</li>
                        <li>Iterate and refine based on evaluation results</li>
                    </ol>
                </div>

                <!-- AI Bot Types - Bot Storage & Memory -->
                <div class="integration-content" id="aibot-storage">
                    <h4>Implementing Storage and Memory for AI Bots</h4>
                    <p>Learn how to give your AI bots persistent memory and efficient storage capabilities:</p>

                    <div class="important-note">
                        <h5>💾 Memory & Storage for Smarter Bots</h5>
                        <p>Adding storage and memory allows your bots to:</p>
                        <ul>
                            <li>Remember past conversations and user preferences</li>
                            <li>Store and retrieve documents and knowledge bases</li>
                            <li>Learn and improve from interactions over time</li>
                            <li>Maintain context across multiple sessions</li>
                            <li>Scale to handle large amounts of data efficiently</li>
                        </ul>
                    </div>

                    <p><strong>Types of AI Bot Memory:</strong></p>
                    <ul>
                        <li><strong>Short-Term Memory:</strong> Remembering the current conversation</li>
                        <li><strong>Long-Term Memory:</strong> Storing information across multiple sessions</li>
                        <li><strong>Episodic Memory:</strong> Remembering specific interactions or events</li>
                        <li><strong>Semantic Memory:</strong> Storing factual knowledge and concepts</li>
                        <li><strong>Procedural Memory:</strong> Remembering how to perform specific tasks</li>
                    </ul>

                    <p><strong>Storage Options in the iTaK AI Stack:</strong></p>
                    <table class="model-table">
                        <tr>
                            <th>Storage Type</th>
                            <th>Service</th>
                            <th>Best For</th>
                            <th>Access</th>
                        </tr>
                        <tr>
                            <td>Relational Database</td>
                            <td>Supabase</td>
                            <td>Structured data, user profiles, conversation history</td>
                            <td><a href="http://localhost:8000" target="_blank">Supabase Dashboard</a></td>
                        </tr>
                        <tr>
                            <td>Vector Database</td>
                            <td>Chroma (via Flowise)</td>
                            <td>Semantic search, document retrieval, embeddings</td>
                            <td><a href="http://localhost:3000" target="_blank">Flowise Dashboard</a></td>
                        </tr>
                        <tr>
                            <td>File Storage</td>
                            <td>Docker Volumes</td>
                            <td>Documents, images, raw data files</td>
                            <td><a href="http://localhost:9000" target="_blank">Portainer</a></td>
                        </tr>
                        <tr>
                            <td>Workflow State</td>
                            <td>n8n</td>
                            <td>Process state, execution history, scheduled tasks</td>
                            <td><a href="http://localhost:5678" target="_blank">n8n Dashboard</a></td>
                        </tr>
                    </table>

                    <p><strong>Setting Up Supabase for Bot Memory:</strong></p>
                    <ol>
                        <li>Access Supabase at <a href="http://localhost:8000" target="_blank">http://localhost:8000</a></li>
                        <li>Create a new project and set up your credentials</li>
                        <li>Create tables for your bot's memory needs</li>
                    </ol>

                    <p><strong>Example Memory Schema:</strong></p>
                    <pre><code class="copyable" data-copy="-- User profiles table
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  username TEXT UNIQUE NOT NULL,
  preferences JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Conversation history table
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES users(id),
  title TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Individual messages table
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  conversation_id UUID REFERENCES conversations(id),
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- User-specific facts and memories
CREATE TABLE user_memories (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES users(id),
  memory_type TEXT NOT NULL,
  content TEXT NOT NULL,
  importance INTEGER DEFAULT 1,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_accessed TIMESTAMP WITH TIME ZONE
);">-- User profiles table
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  username TEXT UNIQUE NOT NULL,
  preferences JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Conversation history table
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES users(id),
  title TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Individual messages table
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  conversation_id UUID REFERENCES conversations(id),
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- User-specific facts and memories
CREATE TABLE user_memories (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES users(id),
  memory_type TEXT NOT NULL,
  content TEXT NOT NULL,
  importance INTEGER DEFAULT 1,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_accessed TIMESTAMP WITH TIME ZONE
);</code></pre>

                    <p><strong>Implementing Conversation Memory in Flowise:</strong></p>
                    <ol>
                        <li>Create a new chatflow in Flowise</li>
                        <li>Add a "Memory" node (e.g., Buffer Memory, Message History)</li>
                        <li>Configure the memory to store in Supabase or another database</li>
                        <li>Connect the Memory node to your LLM node</li>
                        <li>Add a unique identifier for each user to maintain separate memories</li>
                    </ol>

                    <p><strong>Setting Up a Vector Store for Knowledge:</strong></p>
                    <ol>
                        <li>In Flowise, add a "Document Loaders" node to import your documents</li>
                        <li>Connect to a "Text Splitter" node to chunk the content</li>
                        <li>Add an "Embeddings" node to convert text to vectors</li>
                        <li>Connect to a "Vector Store" node (e.g., Chroma)</li>
                        <li>Configure persistence settings to save the vector store</li>
                    </ol>

                    <p><strong>Memory Management Strategies:</strong></p>
                    <ul>
                        <li><strong>Summarization:</strong> Periodically summarize long conversations to save space</li>
                        <li><strong>Importance Ranking:</strong> Store memories with an importance score</li>
                        <li><strong>Recency Bias:</strong> Prioritize recent interactions</li>
                        <li><strong>Forgetting Mechanism:</strong> Implement a strategy to forget less important information</li>
                        <li><strong>Contextual Retrieval:</strong> Only retrieve relevant memories for each interaction</li>
                    </ul>

                    <p><strong>Implementing Memory with n8n:</strong></p>
                    <ol>
                        <li>Create a workflow in n8n that triggers when a user interacts with your bot</li>
                        <li>Add a "Supabase" node to store or retrieve memory</li>
                        <li>Use Function nodes to process and format the memory data</li>
                        <li>Add HTTP Request nodes to communicate with your LLM</li>
                        <li>Set up error handling and retry logic for reliability</li>
                    </ol>

                    <p><strong>Example n8n Workflow for Memory Management:</strong></p>
                    <ol>
                        <li>Webhook node: Receive user message</li>
                        <li>Supabase node: Retrieve user profile and conversation history</li>
                        <li>Function node: Format conversation history for context</li>
                        <li>HTTP Request node: Send to Ollama with context</li>
                        <li>Supabase node: Store the new message and response</li>
                        <li>Respond to user with the AI's response</li>
                    </ol>

                    <p><strong>Security Considerations:</strong></p>
                    <ul>
                        <li>Implement proper authentication and authorization</li>
                        <li>Use encryption for sensitive data</li>
                        <li>Regularly backup your databases</li>
                        <li>Implement data retention policies</li>
                        <li>Consider privacy regulations (GDPR, CCPA, etc.)</li>
                    </ul>
                </div>

                <!-- n8n + Hugging Face + Discord Integration -->
                <div class="integration-content" id="n8n-huggingface-discord">
                    <h4>Making Your Own AI Discord Bot (Super Easy!)</h4>
                    <p>Let's create a fun Discord bot that can talk to people using AI! This bot will answer questions in your Discord server using the AI models from your Hugging Face Chat UI.</p>

                    <div class="important-note">
                        <h5>🐍 What Will Our Bot Do?</h5>
                        <p>When we're done, you'll have a bot that can:</p>
                        <ul>
                            <li>Answer questions in your Discord server using AI</li>
                            <li>Respond when someone types <code>!ask</code> or mentions the bot</li>
                            <li>Use the same AI models you have in your Hugging Face Chat UI</li>
                            <li>Help your friends by answering their questions automatically</li>
                        </ul>
                    </div>

                    <p><strong>Before We Start:</strong></p>
                    <ol>
                        <li>You need a Discord account and your own server (or be an admin on someone else's server)</li>
                        <li>Make sure your iTaK AI Stack is running (you should see the dashboard right now!)</li>
                        <li>Check that Hugging Face Chat UI works by opening <a href="http://localhost:3000" target="_blank">http://localhost:3000</a></li>
                        <li>Check that n8n works by opening <a href="http://localhost:5678" target="_blank">http://localhost:5678</a></li>
                    </ol>

                    <p><strong>Step 1: Create Your Discord Bot (Like Making a New Character)</strong></p>
                    <ol>
                        <li>Open your web browser and go to the <a href="https://discord.com/developers/applications" target="_blank">Discord Developer Portal</a></li>
                        <li>Click the big blue button that says "New Application" in the top right corner</li>
                        <li>Give your bot a cool name (like "MyAIHelper" or "BrainBot") and click "Create"</li>
                        <li>Look at the menu on the left side and click on "Bot"</li>
                        <li>Click the blue button that says "Add Bot" and then click "Yes, do it!" when it asks if you're sure</li>
                        <li>Now you'll see a page with settings for your bot. Scroll down to find "Privileged Gateway Intents"</li>
                        <li>Turn ON all three switches:
                            <ul>
                                <li>✅ Presence Intent</li>
                                <li>✅ Server Members Intent</li>
                                <li>✅ Message Content Intent</li>
                            </ul>
                        </li>
                        <li>Scroll back up and find the "Reset Token" button. Click it and then click "Yes, do it!"</li>
                        <li>A secret code will appear. Click the "Copy" button to copy this code - this is your bot's password! Keep it secret and safe!</li>
                        <li>Now click on "OAuth2" in the left menu, then click on "URL Generator"</li>
                        <li>In the "Scopes" section, check the box next to "bot"</li>
                        <li>In the "Bot Permissions" section that appears below, check these boxes:
                            <ul>
                                <li>✅ Read Messages/View Channels</li>
                                <li>✅ Send Messages</li>
                                <li>✅ Read Message History</li>
                            </ul>
                        </li>
                        <li>Scroll down and find the "Generated URL" - click the "Copy" button</li>
                        <li>Open a new browser tab, paste the URL you just copied, and press Enter</li>
                        <li>Choose the server where you want to add your bot, then click "Continue" and "Authorize"</li>
                        <li>Congratulations! Your bot is now in your Discord server! (But it can't do anything yet - we'll fix that next)</li>
                    </ol>

                    <p><strong>Step 2: Make Sure Your AI Chat Is Ready</strong></p>
                    <ol>
                        <li>Open Hugging Face Chat UI by clicking <a href="http://localhost:3000" target="_blank">http://localhost:3000</a></li>
                        <li>Try asking it a question like "What is the capital of France?" to make sure it's working</li>
                        <li>If it answers correctly, great! If not, make sure you've set up a model in the Hugging Face Chat UI</li>
                    </ol>

                    <p><strong>Step 3: Create the Magic Connection in n8n (Like Building a Bridge)</strong></p>
                    <ol>
                        <li>Open n8n by clicking <a href="http://localhost:5678" target="_blank">http://localhost:5678</a></li>
                        <li>If you see a welcome screen, click through it or click "Skip"</li>
                        <li>Click the big "+" button to create a new workflow</li>
                        <li>In the box that appears, type "Discord-AI-Bot" for the name and click "Create"</li>
                        <li>You'll see a big empty canvas. This is where we'll build our bot's brain!</li>
                        <li>Click the "+" button in the top right of the canvas</li>
                        <li>Type "discord" in the search box and click on "Discord" under "Trigger"</li>
                        <li>A Discord node will appear on the canvas. Click on it to set it up</li>
                        <li>Click "Create new" next to Credentials</li>
                        <li>In the popup, paste your bot token (the secret code you copied earlier) into the "Token" field</li>
                        <li>Click "Create" to save your credentials</li>
                        <li>For "Event" select "Message" from the dropdown</li>
                        <li>For "Message Types" select "Create & Update"</li>
                        <li>Click "Done" to save this node</li>
                        <li>Now click the "+" button at the bottom of the Discord node</li>
                        <li>Type "if" in the search box and select the "IF" node</li>
                        <li>In the IF node settings:
                            <ul>
                                <li>For "Condition" select "Boolean" and "Is Not True"</li>
                                <li>For "Value 1" type or paste exactly: <code>{{$node["Discord"].json["author"]["bot"]}}</code></li>
                                <li>This tells our bot not to respond to messages from other bots</li>
                                <li>Click "Done" to save this node</li>
                            </ul>
                        </li>
                        <li>Click the "+" button on the "true" output of the IF node (we only want to continue if the message is NOT from a bot)</li>
                        <li>Type "function" in the search box and select the "Function" node</li>
                        <li>In the Function node, delete any code that's already there</li>
                        <li>Copy and paste this code (click the copy button):
<pre><code class="copyable" data-copy="// Get the message that was sent
const message = $input.item.content;

// Check if someone is talking to our bot
// Either they typed !ask or they mentioned the bot
const isBotCommand = message.startsWith('!ask') ||
                    $input.item.mentions.some(m => m.bot === true);

// If no one is talking to our bot, stop here
if (!isBotCommand) {
  return [];
}

// Get the actual question by removing the !ask part
let question = message;
if (message.startsWith('!ask')) {
  question = message.substring(5).trim();
} else {
  // Remove the bot mention from the question
  $input.item.mentions.forEach(mention => {
    if (mention.bot) {
      question = question.replace(`<@${mention.id}>`, '').trim();
    }
  });
}

// Send the question and some info about who asked it
return {
  question,
  channelId: $input.item.channelId,
  userId: $input.item.author.id,
  username: $input.item.author.username
};">// Get the message that was sent
const message = $input.item.content;

// Check if someone is talking to our bot
// Either they typed !ask or they mentioned the bot
const isBotCommand = message.startsWith('!ask') ||
                    $input.item.mentions.some(m => m.bot === true);

// If no one is talking to our bot, stop here
if (!isBotCommand) {
  return [];
}

// Get the actual question by removing the !ask part
let question = message;
if (message.startsWith('!ask')) {
  question = message.substring(5).trim();
} else {
  // Remove the bot mention from the question
  $input.item.mentions.forEach(mention => {
    if (mention.bot) {
      question = question.replace(`<@${mention.id}>`, '').trim();
    }
  });
}

// Send the question and some info about who asked it
return {
  question,
  channelId: $input.item.channelId,
  userId: $input.item.author.id,
  username: $input.item.author.username
};</code></pre>
                        </li>
                        <li>Click "Done" to save this node</li>
                        <li>Click the "+" button at the bottom of the Function node</li>
                        <li>Type "http request" in the search box and select the "HTTP Request" node</li>
                        <li>In the HTTP Request node settings:
                            <ul>
                                <li>For "Method" select "POST"</li>
                                <li>For "URL" type or copy: <code class="copyable" data-copy="http://host.docker.internal:3000/api/chat">http://host.docker.internal:3000/api/chat</code></li>
                                <li>For "Authentication" select "None"</li>
                                <li>For "Request Format" select "JSON"</li>
                                <li>For "JSON/RAW Parameters" copy and paste this (click the copy button):
<pre><code class="copyable" data-copy="{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Provide concise, accurate responses."
    },
    {
      "role": "user",
      "content": "{{$node[\"Function\"].json[\"question\"]}}"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 500
}">{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful AI assistant. Provide concise, accurate responses."
    },
    {
      "role": "user",
      "content": "{{$node[\"Function\"].json[\"question\"]}}"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 500
}</code></pre>
                                </li>
                                <li>Click "Done" to save this node</li>
                            </ul>
                        </li>
                        <li>Click the "+" button at the bottom of the HTTP Request node</li>
                        <li>Type "discord" in the search box and select "Discord"</li>
                        <li>In this Discord node settings:
                            <ul>
                                <li>For "Authentication" select the same credentials you created earlier</li>
                                <li>For "Resource" select "Message"</li>
                                <li>For "Operation" select "Send"</li>
                                <li>For "Channel ID" copy and paste: <code>{{$node["Function"].json["channelId"]}}</code></li>
                                <li>For "Content" copy and paste: <code>**Response to @{{$node["Function"].json["username"]}}:**\n\n{{$node["HTTP Request"].json["message"]["content"]}}</code></li>
                                <li>Click "Done" to save this node</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>Step 4: Turn On Your Bot and Test It!</strong></p>
                    <ol>
                        <li>Look at the top right of the n8n page and click the "Save" button</li>
                        <li>After saving, click the "Activate" toggle switch next to the Save button</li>
                        <li>The toggle should turn blue, which means your bot is now active!</li>
                        <li>Open Discord and go to the server where you added your bot</li>
                        <li>In any channel, type <code>!ask What is the capital of France?</code> and press Enter</li>
                        <li>Wait a few seconds, and your bot should reply with "Paris is the capital of France" (or something similar)</li>
                        <li>You can also mention your bot by typing @ followed by your bot's name, then asking a question</li>
                        <li>Try different questions to see what your bot can do!</li>
                    </ol>

                    <p><strong>Cool Things You Can Add Later:</strong></p>
                    <ul>
                        <li><strong>Bot Memory:</strong> Make your bot remember previous conversations</li>
                        <li><strong>Different AI Models:</strong> Let users choose which AI model to use</li>
                        <li><strong>Special Commands:</strong> Add commands like <code>!image</code> to generate images</li>
                        <li><strong>Safety Features:</strong> Add rules to prevent misuse of your bot</li>
                    </ul>

                    <p><strong>Using Discord Webhooks (Advanced):</strong></p>
                    <p>Discord has a feature called "webhooks" that can make your bot look fancier, but it requires HTTPS. Here's how to use them:</p>
                    <ol>
                        <li>First, make sure HTTPS is enabled in your iTaK AI Stack</li>
                        <li>In Discord, right-click on a channel and select "Edit Channel"</li>
                        <li>Click on "Integrations" then "Webhooks" and "New Webhook"</li>
                        <li>Give it a name and picture, then click "Copy Webhook URL"</li>
                        <li>In your n8n workflow, replace the final Discord node with an HTTP Request node</li>
                        <li>Set the Method to "POST" and paste your webhook URL</li>
                        <li>For the JSON body, use this (click to copy):
<pre><code class="copyable" data-copy="{
  "content": "**AI Response:**\n\n{{$node[\"HTTP Request\"].json[\"message\"][\"content\"]}}",
  "username": "AI Assistant",
  "avatar_url": "https://i.imgur.com/4M34hi2.png"
}">{
  "content": "**AI Response:**\n\n{{$node[\"HTTP Request\"].json[\"message\"][\"content\"]}}",
  "username": "AI Assistant",
  "avatar_url": "https://i.imgur.com/4M34hi2.png"
}</code></pre>
                        </li>
                    </ol>

                    <p><strong>Troubleshooting:</strong></p>
                    <ul>
                        <li><strong>Bot doesn't respond:</strong> Make sure your workflow is activated in n8n</li>
                        <li><strong>Error messages:</strong> Check that Hugging Face Chat UI is running</li>
                        <li><strong>Bot responds to other bots:</strong> Make sure your IF node is set up correctly</li>
                        <li><strong>Can't find your bot in Discord:</strong> Try inviting it again with the URL</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="services-grid">
            <div class="service-card">
                <div class="card-content">
                    <h3>n8n</h3>
                    <p>Workflow automation tool</p>
                    <p><strong>Default Login:</strong> <code>Create on first login</code></p>
                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="5678">5678</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:5678">http://host.docker.internal:5678</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="n8n">Start</button>
                        <button class="btn restart-btn" data-service="n8n">Restart</button>
                        <button class="btn stop-btn" data-service="n8n">Stop</button>
                    </div>
                    <a href="https://docs.n8n.io/" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:5678" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://n8n.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Hugging Face Chat UI</h3>
                    <p>Chat with AI models using a modern interface</p>
                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="3000">3000</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:3000">http://host.docker.internal:3000</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="huggingface-chat-ui">Start</button>
                        <button class="btn restart-btn" data-service="huggingface-chat-ui">Restart</button>
                        <button class="btn stop-btn" data-service="huggingface-chat-ui">Stop</button>
                    </div>
                    <a href="https://huggingface.co/docs" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:3000" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://huggingface.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Ollama</h3>
                    <p>Run large language models locally</p>
                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="11434">11434</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:11434">http://host.docker.internal:11434</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="ollama">Start</button>
                        <button class="btn restart-btn" data-service="ollama">Restart</button>
                        <button class="btn stop-btn" data-service="ollama">Stop</button>
                    </div>
                    <a href="https://ollama.com/docs" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:11434" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://ollama.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>SearXNG</h3>
                    <p>Privacy-respecting metasearch engine</p>

                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="8080">8080</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:8080">http://host.docker.internal:8080</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="searxng">Start</button>
                        <button class="btn restart-btn" data-service="searxng">Restart</button>
                        <button class="btn stop-btn" data-service="searxng">Stop</button>
                    </div>
                    <a href="https://docs.searxng.org/" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:8080" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://searxng.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>YouTube Downloader</h3>
                    <p>Download YouTube videos</p>

                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="8998">8998</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:8998">http://host.docker.internal:8998</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="youtube-dl">Start</button>
                        <button class="btn restart-btn" data-service="youtube-dl">Restart</button>
                        <button class="btn stop-btn" data-service="youtube-dl">Stop</button>
                    </div>
                    <a href="https://github.com/yt-dlp/yt-dlp" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:8998" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://youtube-dl.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Playwright Web Scraper</h3>
                    <p>Advanced web scraping API with browser automation</p>

                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="3333">3333</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:3333">http://host.docker.internal:3333</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="playwright-scraper">Start</button>
                        <button class="btn restart-btn" data-service="playwright-scraper">Restart</button>
                        <button class="btn stop-btn" data-service="playwright-scraper">Stop</button>
                    </div>
                    <a href="https://playwright.dev/docs/intro" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:3333" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://playwright.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Flowise</h3>
                    <p>Build LLM apps with a visual interface</p>

                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="3001">3001</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:3001">http://host.docker.internal:3001</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="flowise">Start</button>
                        <button class="btn restart-btn" data-service="flowise">Restart</button>
                        <button class="btn stop-btn" data-service="flowise">Stop</button>
                    </div>
                    <a href="https://docs.flowiseai.com/" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:3001" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://flowise.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Portainer</h3>
                    <p>Container management UI</p>

                    <p><strong>Default Login:</strong> <code>Create on first login</code></p>
                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="9000">9000</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:9000">http://host.docker.internal:9000</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="portainer">Start</button>
                        <button class="btn restart-btn" data-service="portainer">Restart</button>
                        <button class="btn stop-btn" data-service="portainer">Stop</button>
                    </div>
                    <a href="https://docs.portainer.io/" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:9000" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://portainer.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>

            <div class="service-card">
                <div class="card-content">
                    <h3>Supabase</h3>
                    <p>Open source Firebase alternative</p>
                    <div class="service-status">
                        <span class="status-indicator running"></span> Running
                    </div>
                    <div class="service-info">
                        <p><strong>Port:</strong> <code class="copyable" data-copy="8000">8000</code> <span class="port-protocol">TCP</span></p>
                        <p><strong>Docker IP:</strong> <code class="copyable" data-copy="http://host.docker.internal:8000">http://host.docker.internal:8000</code></p>
                    </div>
                </div>
                <div class="card-buttons">
                    <div class="service-controls">
                        <button class="btn start-btn" data-service="supabase-db">Start</button>
                        <button class="btn restart-btn" data-service="supabase-db">Restart</button>
                        <button class="btn stop-btn" data-service="supabase-db">Stop</button>
                    </div>
                    <a href="https://supabase.com/docs" class="btn docs-btn" target="_blank">Documentation</a>
                    <div class="service-links">
                        <a href="http://localhost:8000" class="service-link http-link" target="_blank">HTTP (Local)</a>
                        <a href="https://supabase.localhost" class="service-link https-link" target="_blank">HTTPS (Private)</a>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Floating navigation buttons -->
    <div class="floating-nav">
        <button id="scroll-to-top" title="Scroll to Top"><i class="fas fa-arrow-up"></i></button>
        <button id="scroll-to-bottom" title="Scroll to Bottom"><i class="fas fa-arrow-down"></i></button>
    </div>

    <footer>
        <div class="container">
            <p>iTaK AI Stack | Created by <a href="https://www.youtube.com/@David-Patton" target="_blank" class="author-link">David Patton</a> | Running on port 411 | All services are running locally on your machine</p>
        </div>
    </footer>

    <script src="dashboard.js"></script>
    <script>
        // Copy functionality
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize copyable elements
            initCopyableElements();

            // Get external IP on page load
            refreshExternalIP();

            // Initialize service integrations
            selectService('n8n');

            // Initialize theme from localStorage
            initTheme();
        });

        function initTheme() {
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) {
                document.documentElement.setAttribute('data-theme', savedTheme);
            }
        }

        function toggleTheme() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';

            document.documentElement.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        }

        function initCopyableElements() {
            const copyables = document.querySelectorAll('.copyable');

            copyables.forEach(element => {
                element.addEventListener('click', function() {
                    const textToCopy = this.getAttribute('data-copy') || this.textContent;

                    navigator.clipboard.writeText(textToCopy).then(() => {
                        // Visual feedback
                        this.classList.add('copied');

                        // Remove the 'copied' class after 2 seconds
                        setTimeout(() => {
                            this.classList.remove('copied');
                        }, 2000);
                    }).catch(err => {
                        console.error('Could not copy text: ', err);
                    });
                });
            });
        }

        function refreshExternalIP() {
            const ipElement = document.getElementById('external-ip');
            ipElement.textContent = 'Fetching...';

            fetch('https://api.ipify.org?format=json')
                .then(response => response.json())
                .then(data => {
                    ipElement.textContent = data.ip;
                    ipElement.setAttribute('data-copy', data.ip);
                })
                .catch(error => {
                    console.error('Error fetching IP:', error);
                    ipElement.textContent = 'Error fetching IP';
                });
        }

        function toggleHideIP() {
            const ipElement = document.getElementById('external-ip');
            const hideButton = document.querySelector('.hide-ip-btn');

            if (ipElement.classList.contains('ip-hidden')) {
                ipElement.classList.remove('ip-hidden');
                hideButton.textContent = 'Hide IP';
            } else {
                ipElement.classList.add('ip-hidden');
                hideButton.textContent = 'Show IP';
            }
        }

        function selectServiceFromDropdown() {
            const dropdown = document.getElementById('service-dropdown');
            const selectedService = dropdown.value;
            if (selectedService) {
                selectService(selectedService);
            }
        }

        function selectService(service) {
            // Update dropdown if it exists
            const dropdown = document.getElementById('service-dropdown');
            if (dropdown) {
                dropdown.value = service;
            }

            // Hide all tab groups
            const allTabs = document.querySelectorAll('.integration-tabs');
            allTabs.forEach(tab => {
                tab.style.display = 'none';
            });

            // Show the selected service's tabs
            const selectedTabs = document.getElementById(`${service}-tabs`);
            if (selectedTabs) {
                selectedTabs.style.display = 'flex';

                // Get all tabs for this service
                const tabs = selectedTabs.querySelectorAll('.integration-tab');

                // Find active tab or use first tab
                let activeTab = selectedTabs.querySelector('.integration-tab.active');
                if (!activeTab && tabs.length > 0) {
                    // No active tab, activate the first one
                    activeTab = tabs[0];
                    activeTab.classList.add('active');
                }

                // Show the content for the active tab
                if (activeTab) {
                    const match = activeTab.getAttribute('onclick').match(/'([^']+)'/);
                    if (match && match[1]) {
                        showIntegration(match[1]);
                    }
                }
            }
        }

        function showIntegration(id) {
            // Hide all integration contents
            const contents = document.querySelectorAll('.integration-content');
            contents.forEach(content => {
                content.classList.remove('active');
            });

            // Deactivate all tabs
            const tabs = document.querySelectorAll('.integration-tab');
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });

            // Show the selected content
            document.getElementById(id).classList.add('active');

            // Activate the clicked tab
            const clickedTab = document.querySelector(`[onclick="showIntegration('${id}')"]`);
            clickedTab.classList.add('active');
        }

        function toggleCollapse(id) {
            const content = document.getElementById(id);
            const button = document.querySelector(`[onclick="toggleCollapse('${id}')"]`);

            if (content.classList.contains('collapsed')) {
                // Expand
                content.classList.remove('collapsed');
                button.classList.remove('collapsed');
                button.querySelector('.collapse-icon').textContent = '▼';
            } else {
                // Collapse
                content.classList.add('collapsed');
                button.classList.add('collapsed');
                button.querySelector('.collapse-icon').textContent = '▶';
            }
        }
    </script>
    <script>
        // Function to update IP addresses in the links
        function updateNetworkLinks() {
            fetch('/api/ip')
                .then(response => response.text())
                .then(ip => {
                    const networkLinks = document.querySelectorAll('[data-network-link]');
                    networkLinks.forEach(link => {
                        const url = link.getAttribute('data-network-link');
                        link.href = url.replace('IP_ADDRESS', ip);
                        link.textContent = url.replace('IP_ADDRESS', ip);
                    });
                })
                .catch(error => console.error('Error fetching IP:', error));
        }

        // Call the function when the page loads
        document.addEventListener('DOMContentLoaded', updateNetworkLinks);
    </script>
    <script>
        // Scroll navigation buttons
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scroll-to-top');
            const scrollToBottomBtn = document.getElementById('scroll-to-bottom');

            // Scroll to top button
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });

            // Scroll to bottom button
            scrollToBottomBtn.addEventListener('click', function() {
                window.scrollTo({
                    top: document.body.scrollHeight,
                    behavior: 'smooth'
                });
            });

            // Show/hide buttons based on scroll position
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.style.display = 'flex';
                } else {
                    scrollToTopBtn.style.display = 'none';
                }

                // Calculate if we're near the bottom
                const scrollPosition = window.scrollY + window.innerHeight;
                const nearBottom = document.body.scrollHeight - scrollPosition < 300;

                if (nearBottom) {
                    scrollToBottomBtn.style.display = 'none';
                } else {
                    scrollToBottomBtn.style.display = 'flex';
                }
            });

            // Initial state
            scrollToTopBtn.style.display = 'none';
            if (window.scrollY + window.innerHeight >= document.body.scrollHeight - 300) {
                scrollToBottomBtn.style.display = 'none';
            }
        });
    </script>
</body>
</html>




